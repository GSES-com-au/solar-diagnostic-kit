{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Libraries import\n",
    "# ========================================================\n",
    "\n",
    "import numpy as np\n",
    "import boto3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "import os\n",
    "\n",
    "import pytz\n",
    "\n",
    "import pvlib\n",
    "from pvlib import irradiance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = AWS Credentials\n",
    "# ========================================================\n",
    "\n",
    "PROD_AWS_PROFILE = \"gsesami-prod\"\n",
    "AWS_REGION = \"ap-southeast-2\"\n",
    "\n",
    "prod_session = boto3.session.Session(profile_name=PROD_AWS_PROFILE)\n",
    "\n",
    "prod_client = prod_session.client(\n",
    "    \"timestream-query\", region_name=AWS_REGION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Querying TimeStream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Monitor ID, Site ID, and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all sites\n",
    "sites_list = pd.read_csv('./input_data/Site_List.csv')\n",
    "# Reading all monitors\n",
    "monitors_list = pd.read_csv('./input_data/Monitors_List.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor ID\n",
    "MID = '282136'\n",
    "\n",
    "# Displaying MID\n",
    "MID_full = str(\"MNTR|\" + MID)\n",
    "print(f'Analysing {MID_full}')\n",
    "\n",
    "# Get site_id from MID\n",
    "site_id = monitors_list.loc[monitors_list['source'] == str(\"MNTR|\" + MID), 'siteId'].iloc[0]\n",
    "print(f'Included under {site_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the monitor list is of type string:\n",
    "monitors_list['source'] = monitors_list['source'].astype(str)\n",
    "\n",
    "# filtering the dataframe based on the monitor in question:\n",
    "monitor_row = monitors_list.loc[monitors_list['source'] == MID_full]\n",
    "\n",
    "# If the monitor ID exists, isolate each useful variable:\n",
    "if not monitor_row.empty:\n",
    "    # latitude\n",
    "    # Notice that the data output here for latitude is really weird,\n",
    "    # Have to fix it a bit:\n",
    "    mid_latitude = float(monitor_row['latitude'].values[0])\n",
    "    # longitude:\n",
    "    mid_longitude = float(monitor_row['longitude'].values[0])\n",
    "    # loss:\n",
    "    mid_loss = float( 1 - monitor_row['loss'].values[0] )\n",
    "    # manufacturer api:\n",
    "    mid_manufacturerApi = monitor_row['manufacturerApi'].values[0] \n",
    "    # pvSize:\n",
    "    mid_pvsizewatt = monitor_row['pvSizeWatt'].values[0]\n",
    "    # tilt:\n",
    "    mid_tilt = float(monitor_row['tilt'].values[0])\n",
    "    # weatherstationid:\n",
    "    mid_weatherStationId = monitor_row['weatherStationId'].values[0]\n",
    "    # Azymuth:\n",
    "    mid_azimuth = float(monitor_row['azimuth'].values[0])\n",
    "        \n",
    "    print(f'Latitude: {mid_latitude}')\n",
    "    print(f'Longitude: {mid_longitude}')\n",
    "    print(f'Loss: {mid_loss}')\n",
    "    print(f'pvSizeWatt: {mid_pvsizewatt}')\n",
    "    print(f'ManufacturerApi: {mid_manufacturerApi}')\n",
    "    print(f'Tilt: {mid_tilt}')\n",
    "    print(f'WeatherStationId: {mid_weatherStationId}')\n",
    "    print(f'Azimuth: {mid_azimuth}')\n",
    "else:\n",
    "    print(f\"No data found for the Monitor ID {MID}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TIME PERIOD #######\n",
    "time_start = '2023-04-20'\n",
    "time_end = '2023-04-25'\n",
    "\n",
    "# Setting date_end to today\n",
    "#today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "#time_end = today\n",
    "\n",
    "# I'm geting these from the manually labelled faults, I'll need it to be a string to run the SQL query\n",
    "# However, I want to add 1 day before and after these dates for a proper plot, so:\n",
    "\n",
    "## Gotta convert it to datetime:\n",
    "time_start = datetime.datetime.strptime(time_start, '%Y-%m-%d')\n",
    "time_end = datetime.datetime.strptime(time_end, '%Y-%m-%d')\n",
    "\n",
    "## Subtract one day\n",
    "time_start = time_start - timedelta(days=1)\n",
    "time_end = time_end + timedelta(days=1)\n",
    "\n",
    "# Convertting again from datetime object back to string\n",
    "time_start = time_start.strftime('%Y-%m-%d')\n",
    "time_end = time_end.strftime('%Y-%m-%d')\n",
    "\n",
    "# Checking timezone\n",
    "timezone_value = sites_list[sites_list['source'] == site_id].iloc[0]['timezone']\n",
    "\n",
    "# time_endtz = datetime.datetime.fromisoformat(time_end_short)\n",
    "time_starttz = pytz.timezone('UTC').localize(datetime.datetime.strptime(time_start, '%Y-%m-%d'))\n",
    "time_endtz = pytz.timezone('UTC').localize(datetime.datetime.strptime(time_end, '%Y-%m-%d'))\n",
    "\n",
    "print(f'Current analysis being performed for days {time_start} to {time_end}')\n",
    "print(f'This monitor is located at: {timezone_value}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Helper functions to read metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metric(time_start, time_end, measure_name, MID):\n",
    "    \"\"\"\n",
    "    read raw data from the AWS database\n",
    "    :param time_start: time start, e.g., '2022-10-02'\n",
    "    :param time_end: time end, e.g., '2023-04-05'\n",
    "    :param measure_name: measurement metric, e.g.,'Gen.W'\n",
    "    :param MID: monitor id\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT time, measure_value::bigint\n",
    "                    FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                    WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                    AND MID = '\"\"\" + MID + \"\"\"'\n",
    "                    AND time BETWEEN '\"\"\" + time_start + \"\"\"'\n",
    "                    AND '\"\"\" + time_end + \"\"\"' \"\"\"\n",
    "\n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(timeid, measure_name, data_values, timezone_value):\n",
    "    \"\"\"\n",
    "    change the time zone\n",
    "    :param timeid: time read from the AWS\n",
    "    :param measure_name:\n",
    "    :param data_values: value read from the ASW\n",
    "    :param timezone_value: time zone\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    timeid = pd.to_datetime(timeid)\n",
    "    if timeid.tzinfo is None:\n",
    "        print('this is not tz-aware')\n",
    "        if timezone_value is not None:\n",
    "            timeid = timeid.tz_localize('UTC').tz_convert(timezone_value)\n",
    "        else:\n",
    "            print('no timezone in the table')\n",
    "            timeid = timeid.tz_localize('UTC').tz_convert('Australia/Sydney')\n",
    "    else:\n",
    "        print('this is tz-aware')\n",
    "    data = pd.DataFrame(data={'time':timeid, measure_name: data_values})\n",
    "    data.sort_values('time', inplace=True)\n",
    "    # data.set_index('time', inplace=True)\n",
    "    data[measure_name] = data[measure_name].astype(float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_tz(timeid):\n",
    "    # print('rawtimeid:', timeid)\n",
    "    tzinfo_str = timeid[0].tzinfo\n",
    "    hour_offset = tzinfo_str.utcoffset(datetime.datetime(2022,1,1))\n",
    "    hms = str(hour_offset).split(':')\n",
    "    time_modified = timeid + datetime.timedelta(hours=int(hms[0]), minutes=int(hms[1]), seconds=int(hms[2]))\n",
    "    time_utc = time_modified.dt.tz_convert('UTC')\n",
    "    # print('modified:', time_utc)\n",
    "    return time_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Helper functions to get low-cloudiness days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metric_site(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe_site(timeid, measure_name, data_values):\n",
    "    # ============== Check if there is data available for the pv system =============\n",
    "    if len(timeid)!=0:\n",
    "        timeid = pd.to_datetime(timeid)\n",
    "        if timeid.tzinfo is None:\n",
    "            print('this is not tz-aware')\n",
    "            if timezone_value is not None:\n",
    "                timeid = timeid.tz_localize('UTC').tz_convert(timezone_value)\n",
    "                # timeid = timeid.tz_localize(timezone_list[i])\n",
    "            else:\n",
    "                print('no timezone in the table')\n",
    "                timeid = timeid.tz_localize('UTC').tz_convert('Australia/Sydney')\n",
    "                # timeid = timeid.tz_localize('Australia/Sydney')\n",
    "        else:\n",
    "            print('this is tz-aware')\n",
    "        \n",
    "        timesort = timeid.sort_values()\n",
    "        data = pd.DataFrame(data={'time':timeid, measure_name: data_values})\n",
    "        data.sort_values('time', inplace=True)\n",
    "        data.set_index('time', inplace=True)\n",
    "        data[measure_name] = data[measure_name].astype(float)\n",
    "    else:\n",
    "        data = pd.DataFrame(data_values, index=timeid, columns=[measure_name])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# = Merging clear skies and expected\n",
    "# ==================================\n",
    "\n",
    "def merge_clear_expe(df1, df2):\n",
    "    df_merged = df1.join(df2)\n",
    "    df_merged['expected_over_clear'] =  (df_merged['Irrad.kWh.m2.Daily'] / df_merged['EnergyYield.kWh.Daily'] * 100).round(0)\n",
    "    df_merged['date'] =  df_merged.index\n",
    "    return df_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Setting up a 5minutes datetime dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index5min = pd.date_range(start=time_starttz, end=time_endtz, freq='5min').tz_convert('UTC')\n",
    "df_5min = pd.DataFrame(index=np.arange(len(time_index5min)))\n",
    "df_5min['time'] = time_index5min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reading AC Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Building the Gen.W dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Reading P(AC) total from AWS TimeStream\n",
    "# = Metric is Gen.W \n",
    "# ========================================================\n",
    "\n",
    "measure_name = 'Gen.W'\n",
    "timeid, data_values = read_metric(time_start, time_end, measure_name, MID)\n",
    "df_genW = build_dataframe(timeid, measure_name, data_values, timezone_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Adjusting the df_genW to have 5 minutes increments\n",
    "# ========================================================\n",
    "\n",
    "# Convert the 'time' column in df_5min to timezone\n",
    "df_5min['time'] = df_5min['time'].dt.tz_convert(timezone_value)\n",
    "\n",
    "df_genW = pd.merge_asof(df_5min, df_genW, on=\"time\")\n",
    "\n",
    "# Getting the first valid index:\n",
    "first_valid_index = df_genW['Gen.W'].first_valid_index()\n",
    "df_genW = df_genW[first_valid_index:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_5minGenw(df, time_start, time_end):\n",
    "    # plot the data\n",
    "    fig = plt.figure(figsize=(24,3)) \n",
    "    plt.plot(df['time'], df['Gen.W'])\n",
    "    plt.title(f\"Gen.W Data [5 minutes] from {time_start} to {time_end}\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Gen.W')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Create the desired directory if it doesn't exist\n",
    "    dir_path = f\"./recurrent_faults_plots/5MIN_RAWGENW/{MID}\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    # Save it\n",
    "    plt.savefig(f\"{dir_path}/{time_start}_{time_end}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_5minGenw(df_genW, time_start, time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure there's no negative generation\n",
    "## I'll be saving a copy of the original Gen.W to include an analysis of negative generation afterwards\n",
    "\n",
    "df_genW['original_genW'] = df_genW['Gen.W']\n",
    "\n",
    "# Now clipping\n",
    "df_genW['Gen.W'] = df_genW['Gen.W'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fig = plt.figure(figsize=(24,3)) \n",
    "plt.plot(df_genW['time'], df_genW['Gen.W'])\n",
    "plt.title(f\"Gen.W Data [5 minutes] from {time_start} to {time_end}\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Gen.W')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Plotting faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the start and end of faults\n",
    "start_faults = pd.Timestamp(time_start)\n",
    "end_faults = pd.Timestamp(time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'time' column to datetime\n",
    "# df_genW['time'] = pd.to_datetime(df_genW['time'])\n",
    "\n",
    "# Creating a copy of the 'time' column\n",
    "df_genW['time_copy'] = df_genW['time']\n",
    "\n",
    "# Setting the 'time' column as the index\n",
    "df_genW = df_genW.set_index('time')\n",
    "\n",
    "# Resampling the dataframe hourly\n",
    "df_genW_resampled = df_genW.resample('H').mean()\n",
    "\n",
    "# Reseting the index\n",
    "df_genW_resampled.reset_index(level=0, inplace=True)\n",
    "df_genW.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataframe\n",
    "df_sequence_fault = df_genW_resampled[(df_genW_resampled['time'].dt.date >= start_faults.date()) & (df_genW_resampled['time'].dt.date <= end_faults.date())]\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(24,3)) \n",
    "plt.plot(df_sequence_fault['time'], df_sequence_fault['Gen.W'])\n",
    "plt.title(f\"Gen.W Data [hourly] from {start_faults} to {end_faults}\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Gen.W')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Plotting single days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_to_analyse = start_faults + timedelta(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day = df_genW_resampled[df_genW_resampled['time'].dt.date == day_to_analyse.date()]\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(12,6)) \n",
    "plt.plot(df_day['time'], df_day['Gen.W'])\n",
    "plt.title(f\"Gen.W Data for {day_to_analyse}, 2023\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Gen.W')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Plotting a full week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data for the 7-day period\n",
    "start_date = day_to_analyse - DateOffset(days=3)\n",
    "end_date = day_to_analyse + DateOffset(days=3)\n",
    "\n",
    "df_7_days = df_genW_resampled[(df_genW_resampled['time'].dt.date >= start_date.date()) & (df_genW_resampled['time'].dt.date <= end_date.date())]\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(24,3)) \n",
    "plt.plot(df_7_days['time'], df_7_days['Gen.W'])\n",
    "plt.title(f\"Gen.W Data from {start_date.date()} to {end_date.date()}, 2023\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Gen.W')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Getting Low-Cloudiness days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# = Reading EnergyYield.kWh.Daily from AWS TimeStream\n",
    "# ===================================================\n",
    "\n",
    "# making sure the site ID has no prefix\n",
    "site_id = site_id.removeprefix('SITE|')\n",
    "\n",
    "# Reading:\n",
    "measure_name = 'EnergyYield.kWh.Daily'\n",
    "timeid, data_values = read_metric_site(time_start, time_end, measure_name, site_id)\n",
    "df_clear = build_dataframe_site(timeid, measure_name, data_values)\n",
    "\n",
    "# ================================================\n",
    "# = Reading Irrad.kWh.m2.Daily from AWS TimeStream\n",
    "# ================================================\n",
    "\n",
    "measure_name = 'Irrad.kWh.m2.Daily'\n",
    "timeid, data_values = read_metric_site(time_start, time_end, measure_name, site_id)\n",
    "df_expected = build_dataframe_site(timeid, measure_name, data_values)\n",
    "\n",
    "# Fixing it as a float:\n",
    "df_expected['Irrad.kWh.m2.Daily'] = df_expected['Irrad.kWh.m2.Daily'].astype(float)\n",
    "\n",
    "# ==================================================\n",
    "# = Reading Production.kWh.Daily from AWS TimeStream\n",
    "# ==================================================\n",
    "\n",
    "measure_name = 'Production.kWh.Daily'\n",
    "timeid, data_values = read_metric_site(time_start, time_end, measure_name, site_id)\n",
    "df_production = build_dataframe_site(timeid, measure_name, data_values)\n",
    "# Fixing it as float\n",
    "df_production['Production.kWh.Daily'] = df_production['Production.kWh.Daily'].astype(float)\n",
    "\n",
    "\n",
    "# Merging clear and expected:\n",
    "df_merged = df_clear.join(df_expected)\n",
    "# Merging (clear and expected) and production\n",
    "df_merged = df_merged.join(df_production)\n",
    "\n",
    "# Getting the performance ratio:\n",
    "df_merged['expected_over_clear'] =  (df_merged['Irrad.kWh.m2.Daily'] / df_merged['EnergyYield.kWh.Daily'] * 100).round(0)\n",
    "\n",
    "# Getting this extra column flor plotting:\n",
    "df_merged['date'] =  df_merged.index.date\n",
    "\n",
    "# ========================================================\n",
    "# = Checking values above a certain threshold when comparing clear skies and expected\n",
    "# ========================================================\n",
    "\n",
    "# Define the threshold for low cloudiness days:\n",
    "threshold_low_cloudiness = 80\n",
    "\n",
    "# Make it low_cloudiness aware:\n",
    "df_merged.loc[df_merged['expected_over_clear'] >= threshold_low_cloudiness, 'is_low_cloudiness_day'] = True \n",
    "df_merged.loc[df_merged['expected_over_clear'] < threshold_low_cloudiness, 'is_low_cloudiness_day'] = False\n",
    "\n",
    "df_site = df_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data from PVLib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Getting sunrise and sunset times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every day only has 1 sunrise and 1 sunset, so we can go with uique days:\n",
    "unique_dates = df_genW['time'].dt.date.unique()\n",
    "\n",
    "# initialising:\n",
    "sun_info = {}\n",
    "# iterating over my array of uniquedates:\n",
    "for date in unique_dates:\n",
    "    # pvlib expects a localised value, not an iterable list, so I'm wrapping my date timestamp:\n",
    "    localized_date = pd.DatetimeIndex([pd.Timestamp(date).tz_localize(timezone_value)]) \n",
    "    # getting the results, which is a 1 row dataframe with 3 columns:\n",
    "    sun_results = pvlib.solarposition.sun_rise_set_transit_spa(localized_date, mid_latitude, mid_longitude)\n",
    "    sun_info[date] = sun_results.values[0] \n",
    "\n",
    "# Convert dictionary to a DataFrame:\n",
    "sun_info_df = pd.DataFrame.from_dict(sun_info, orient='index', columns=['sunrise', 'sunset', 'transit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Merging with df_genW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a date column for the merge:\n",
    "df_genW['date'] = df_genW['time'].dt.date\n",
    "\n",
    "# Reset index in sun_info_df and rename the index column as 'date'\n",
    "sun_info_df = sun_info_df.reset_index().rename(columns={'index':'date'})\n",
    "\n",
    "# Convert the 'date' column to datetime in both dataframes\n",
    "df_genW['date'] = pd.to_datetime(df_genW['date'])\n",
    "sun_info_df['date'] = pd.to_datetime(sun_info_df['date'])\n",
    "\n",
    "# Merge df_genW with sun_info_df\n",
    "df_genW = pd.merge(df_genW, sun_info_df, on='date', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Adding GHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' column to datetime, to be sure:\n",
    "df_genW['time'] = pd.to_datetime(df_genW['time'])\n",
    "\n",
    "# Create a copy of 'time' column before setting it as index, I'll need this for the labelling\n",
    "df_genW['time_copy'] = df_genW['time']\n",
    "\n",
    "# Set 'time' column as index\n",
    "df_genW.set_index('time', inplace=True)\n",
    "\n",
    "# Running pvlib to get GHI:\n",
    "location = pvlib.location.Location(mid_latitude, mid_longitude, tz=timezone_value)\n",
    "df_genW['clear_sky_ghi'] = location.get_clearsky(df_genW.index, model='ineichen')['ghi']\n",
    "\n",
    "# Renaming time_copy to time again:\n",
    "df_genW.rename(columns={'time_copy': 'time'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Clear Sky Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://pvlib-python.readthedocs.io/en/stable/user_guide/clearsky.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irradiance(loc, times, tilt, surface_azimuth):\n",
    "    # Generate clearsky data using the Ineichen model, which is the default\n",
    "    # The get_clearsky method returns a dataframe with values for GHI, DNI,and DHI\n",
    "    clearsky = loc.get_clearsky(times)\n",
    "    # Get solar azimuth and zenith to pass to the transposition function\n",
    "    solar_position = loc.get_solarposition(times=times)\n",
    "    # Use the get_total_irradiance function to transpose the GHI to POA\n",
    "    POA_irradiance = irradiance.get_total_irradiance(\n",
    "        surface_tilt=tilt,\n",
    "        surface_azimuth=surface_azimuth,\n",
    "        dni=clearsky['dni'],\n",
    "        ghi=clearsky['ghi'],\n",
    "        dhi=clearsky['dhi'],\n",
    "        solar_zenith=solar_position['apparent_zenith'],\n",
    "        solar_azimuth=solar_position['azimuth'])\n",
    "    # Return DataFrame with only GHI and POA\n",
    "    return pd.DataFrame({'GHI': clearsky['ghi'],\n",
    "                         'POA': POA_irradiance['poa_global']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a dataframe with local info:\n",
    "time_index5min_local = pd.date_range(start=pd.to_datetime(time_start).tz_localize(timezone_value), end=pd.to_datetime(time_end).tz_localize(timezone_value), freq='5min')\n",
    "\n",
    "# Getting the location\n",
    "loc = pvlib.location.Location(mid_latitude, mid_longitude, tz=timezone_value)\n",
    "pvlib_irr_pre = get_irradiance(loc, time_index5min_local, mid_tilt, mid_azimuth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting for size and loss\n",
    "pvlib_irr_pre['POA'] = pvlib_irr_pre['POA'] * mid_pvsizewatt * mid_loss / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes\n",
    "merged_df = pd.merge(df_genW, pvlib_irr_pre, left_index=True, right_index=True)\n",
    "\n",
    "# Assign the column\n",
    "merged_df['theoretical_clear-sky_generation.W'] = merged_df['POA']\n",
    "\n",
    "# If you want the result back in df_genW, you can do:\n",
    "df_genW = merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1. Resampling hourly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genW_resample = df_genW.copy()  # create a copy of the original dataframe\n",
    "\n",
    "df_genW_resample['time'] = pd.to_datetime(df_genW_resample['time']) # convert time to datetime if it's not\n",
    "df_genW_resample.set_index('time', inplace=True) # set time as index\n",
    "\n",
    "# Create new df with resampling\n",
    "df_genW_resample = df_genW_resample.resample('H').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Overlapping Clear sky values and measured generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1. Before shifting the timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_7_days['time'].min()\n",
    "end_date = df_7_days['time'].max()\n",
    "\n",
    "df_sliced = df_genW_resample.loc[start_date:end_date]\n",
    "\n",
    "plt.figure(figsize=(24, 3))\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['Gen.W'], label = 'Gen.W')\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['theoretical_clear-sky_generation.W'], label = 'Theoretical clear sky generation')\n",
    "\n",
    "plt.title(f'Gen.W and Theoretical Clear Sky Generation Over Time - With Azimuth = {mid_azimuth} and Tilt = {mid_tilt}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2. Shifting the dataframe to overlap generation and clear sky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that before shifting I want to apply a threshold on the aggregate hourly dataframe before shifting, to avoid situations in which there's more rows of measured than theoretical\n",
    "\n",
    "threshold_minimum_generation = df_genW_resample['Gen.W'].max() * 0.01\n",
    "threshold_minimum_theoretical = df_genW_resample['theoretical_clear-sky_generation.W'].max() * 0.01\n",
    "\n",
    "df_genW_resample.loc[df_genW_resample['Gen.W'] <  threshold_minimum_generation, 'Gen.W'] = 0\n",
    "df_genW_resample.loc[df_genW_resample['theoretical_clear-sky_generation.W'] <  threshold_minimum_theoretical, 'theoretical_clear-sky_generation.W'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_minimum_theoretical = df_genW_resample['theoretical_clear-sky_generation.W'].max() * 0.01\n",
    "df_genW_resample.loc[df_genW_resample['theoretical_clear-sky_generation.W'] <  threshold_minimum_theoretical, 'theoretical_clear-sky_generation.W'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that cadence_of_observation is in minutes.\n",
    "\n",
    "def shift_timeseries(df,cadence_of_observation = 5):\n",
    "    # Finding the first non-zero generation timestamp for each DataFrame\n",
    "    ## Measured\n",
    "    start_genW = df[df['Gen.W'] != 0]['time'].iloc[0]\n",
    "    ## Clear Skies\n",
    "    start_theoretical = df[df['theoretical_clear-sky_generation.W'] != 0]['time'].iloc[0]\n",
    "\n",
    "    # Compute time difference in hours\n",
    "    time_diff = (start_theoretical - start_genW).total_seconds() / 3600\n",
    "\n",
    "    # Converting timedelta into minutes\n",
    "    shift_minutes = timedelta(hours=time_diff).seconds // 60\n",
    "\n",
    "    print(f'shifting the timeseries by {shift_minutes} minutes')\n",
    "\n",
    "    shift_periods = int(shift_minutes / cadence_of_observation)\n",
    "\n",
    "    # Shift 'Gen.W' column in df_genW_copy\n",
    "    df['Gen.W'] = df['Gen.W'].shift(shift_periods)\n",
    "\n",
    "    # Fill NaN values (if any) with 0 after shifting\n",
    "    df['Gen.W'].fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genW_resample['time'] = df_genW_resample.index\n",
    "df_genW_resample = shift_timeseries(df_genW_resample, cadence_of_observation=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the original dataframe\n",
    "df_copy = df_genW_resample.copy()  \n",
    "df_copy['time'] = pd.to_datetime(df_copy['time']) \n",
    "# Setting the index\n",
    "df_copy.set_index('time', inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3. Plot after shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_7_days['time'].min()\n",
    "end_date = df_7_days['time'].max()\n",
    "\n",
    "df_to_plot = df_genW_resample.copy(deep=True)\n",
    "df_to_plot = df_to_plot.resample('H').mean()\n",
    "\n",
    "df_sliced = df_to_plot.loc[start_date:end_date]\n",
    "\n",
    "plt.figure(figsize=(24, 3))\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['Gen.W'], label = 'Gen.W')\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['theoretical_clear-sky_generation.W'], label = 'Theoretical clear sky generation')\n",
    "\n",
    "plt.title(f'[After shifting] Gen.W and Theoretical Clear Sky Generation Over Time - With Azimuth = {mid_azimuth} and Tilt = {mid_tilt}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.4. Delta after shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genW_resample['time'] = df_genW_resample.index\n",
    "\n",
    "start_date = df_7_days['time'].min()\n",
    "end_date = df_7_days['time'].max()\n",
    "\n",
    "df_slice = df_genW_resample[start_date:end_date]\n",
    "\n",
    "df_slice['time'] = pd.to_datetime(df_slice['time']) \n",
    "\n",
    "df_slice['delta-clear-gen'] = df_slice['Gen.W'] - df_slice['theoretical_clear-sky_generation.W']\n",
    "\n",
    "# Creating 2 subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 6), sharex=True) \n",
    "\n",
    "# plot 'Gen.W' and 'theoretical_clear-sky_generation.W' against 'time' in the first subplot\n",
    "sns.lineplot(ax=axes[0], x='time', y='Gen.W', data=df_slice, label='Gen.W', color='blue')\n",
    "sns.lineplot(ax=axes[0], x='time', y='theoretical_clear-sky_generation.W', data=df_slice, label='Theoretical Clear Sky Generation', color='orange')\n",
    "axes[0].legend()\n",
    "\n",
    "# plot 'delta-clear-gen' against 'time' in the second subplot\n",
    "sns.lineplot(ax=axes[1], x='time', y='delta-clear-gen', data=df_slice, label='Delta Clear Gen', color='green')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.title(f'[After shifting] Delta of Gen.W and Theoretical Clear Sky Generation Over Time')\n",
    "\n",
    "plt.tight_layout()  # adjust subplot params so that the subplots fit into the figure area\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.5. Interactive plot after shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the first line\n",
    "fig.add_trace(go.Scatter(x=df_genW_resample['time'], y=df_genW_resample['Gen.W'], mode='lines', name='Gen.W'))\n",
    "\n",
    "# Add the second line\n",
    "fig.add_trace(go.Scatter(x=df_genW_resample['time'], y=df_genW_resample['theoretical_clear-sky_generation.W'], mode='lines', name='Theoretical'))\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Normalising the curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1. Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize columns based on the maximum value of theoretical_clear-sky_generation.W for each day\n",
    "def normalize_daywise(group):\n",
    "\n",
    "    # CUrrently hardcoded Gen.W -> Make sure to change this to a if stament to encompass the cases for when\n",
    "    # On a day Gen.W has a higher max and on a day Theoritical has a higher max\n",
    "\n",
    "    max_val = max(group['Gen.W'].max(), group['theoretical_clear-sky_generation.W'].max())\n",
    "\n",
    "    # Avoiding division by zero and infinite results tehreafter:\n",
    "    max_val = max_val if max_val != 0 else 1\n",
    "    \n",
    "    group['theoretical_clear-sky_generation.W_normalized'] = group['theoretical_clear-sky_generation.W'] / max_val\n",
    "    \n",
    "    group['Gen.W_normalized'] = group['Gen.W'] / max_val\n",
    "    return group\n",
    "\n",
    "# Group by day and apply normalization\n",
    "df_normalized = df_genW_resample.groupby(df_genW_resample.index.date).apply(normalize_daywise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2. Plot after normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_7_days['time'].min()\n",
    "end_date = df_7_days['time'].max()\n",
    "\n",
    "df_to_plot = df_normalized.copy(deep=True)\n",
    "df_to_plot = df_to_plot.resample('H').mean()\n",
    "\n",
    "df_sliced = df_to_plot.loc[start_date:end_date]\n",
    "\n",
    "plt.figure(figsize=(24, 3))\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['Gen.W_normalized'], label = 'Normalised Gen.W')\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['theoretical_clear-sky_generation.W_normalized'], label = 'Normalised Theoretical clear sky generation')\n",
    "\n",
    "plt.title(f'Normalised Gen.W and Normalised Theoretical Clear Sky Generation Over Time - With Azimuth = {mid_azimuth} and Tilt = {mid_tilt}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3. Interative Plot after normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the first line\n",
    "fig.add_trace(go.Scatter(x=df_normalized.index, y=df_normalized['Gen.W_normalized'], mode='lines', name='Gen.W_normalized'))\n",
    "\n",
    "# Add the second line\n",
    "fig.add_trace(go.Scatter(x=df_normalized.index, y=df_normalized['theoretical_clear-sky_generation.W_normalized'], mode='lines', name='theoretical_clear-sky_generation.W_normalized'))\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Stretching theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1. Stretching theoretical with a minimum threshold and MAX values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_theoretical(df):\n",
    "\n",
    "    # I'm initialising an empty dict so I can keep track of the stretch factors:\n",
    "    stretch_factors = {}\n",
    "\n",
    "    # Threshold for theoretical value below which we won't compute the ratio\n",
    "    ## The threshold helps in avoiding unrealistically high stretch factors caused by near-zero values in the denominator.\n",
    "    \n",
    "    ## Currently using 0.7, as 70% of the main normalised value (1)\n",
    "    THRESHOLD = 0.7\n",
    "\n",
    "    # Function to compute and adjust the theoretical curve for each day's group\n",
    "    def compute_stretch(group):\n",
    "\n",
    "        # Check if the input is a DataFrame and if not, return as is\n",
    "        ## I've been getting some weird AttributeErrors here\n",
    "        if not isinstance(group, pd.DataFrame):\n",
    "            return group\n",
    "\n",
    "        # getting the date from each row, so I can keep track of stretch factors\n",
    "        current_date = group.index[0].date()\n",
    "\n",
    "        # Compute ratio where theoretical value is above the threshold\n",
    "        ## I want to make sure that I'm not dividing by tiny values and getting crazy spikes\n",
    "        valid_idxs = group['theoretical_clear-sky_generation.W_normalized'] > THRESHOLD\n",
    "        ratios = (group['Gen.W_normalized'] / group['theoretical_clear-sky_generation.W_normalized'])[valid_idxs]\n",
    "        \n",
    "        # Use maximum ratio as stretch factor\n",
    "        stretch_factor = ratios.max()\n",
    "        \n",
    "        # If the stretch factor is NaN or zero (no valid ratios), set it to 1 to avoid NaN results\n",
    "        # This will keep the value unchanged\n",
    "        stretch_factor = 1 if pd.isna(stretch_factor) or stretch_factor == 0 else stretch_factor\n",
    "\n",
    "        # Storing each stretch factor so I can keep track of it:\n",
    "        stretch_factors[current_date] = stretch_factor\n",
    "\n",
    "        # Multiply each value in the theoretical column by the stretched factor\n",
    "        group['stretched_theoretical'] = group['theoretical_clear-sky_generation.W_normalized'] * stretch_factor\n",
    "        return group\n",
    "\n",
    "    # I'm grouping the datasframe by DAY and applying that function to it\n",
    "    df_stretched = df.groupby(df.index.map(lambda x: x.date())).apply(compute_stretch)\n",
    "\n",
    "    # Convert the stretch_factors dictionary to a DataFrame for better vis\n",
    "    stretch_factors_df = pd.DataFrame(list(stretch_factors.items()), columns=['date', 'stretch_factor'])\n",
    "    # Convert 'date' column to datetime and set as index\n",
    "    stretch_factors_df['date'] = pd.to_datetime(stretch_factors_df['date'])\n",
    "    stretch_factors_df.set_index('date', inplace=True)\n",
    "\n",
    "    return df_stretched, stretch_factors_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stretched, stretch_factors_df = stretch_theoretical(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2. Plotting after stretching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_7_days['time'].min()\n",
    "end_date = df_7_days['time'].max()\n",
    "\n",
    "df_to_plot = df_stretched.copy(deep=True)\n",
    "df_to_plot = df_to_plot.resample('H').mean()\n",
    "\n",
    "df_sliced = df_to_plot.loc[start_date:end_date]\n",
    "\n",
    "plt.figure(figsize=(24, 3))\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['Gen.W_normalized'], label = 'Normalised Gen.W')\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['stretched_theoretical'], label = 'Stretched Theoretical clear sky generation')\n",
    "\n",
    "plt.title(f'Normalised Gen.W and Stretched Theoretical Clear Sky Generation Over Time - With Azimuth = {mid_azimuth} and Tilt = {mid_tilt}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3. Interactive Plotting after stretching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the first line\n",
    "fig.add_trace(go.Scatter(x=df_stretched.index, y=df_stretched['Gen.W_normalized'], mode='lines', name='Gen.W normalised'))\n",
    "\n",
    "# Add the second line\n",
    "fig.add_trace(go.Scatter(x=df_stretched.index, y=df_stretched['stretched_theoretical'], mode='lines', name='Theoretical Stretched'))\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7. Capping Max values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.1. Capping max stretched theoretical based on normalised max gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values of 'theoretical_clear-sky_generation.W' should never be bigger than 'Gen.W'\n",
    "# For now, I'm using Gen.W as the cap\n",
    "df_stretched['Gen.W_normalized'] = np.where(df_stretched['Gen.W_normalized'] > df_stretched['stretched_theoretical'], df_stretched['stretched_theoretical'], df_stretched['Gen.W_normalized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_7_days['time'].min()\n",
    "end_date = df_7_days['time'].max()\n",
    "\n",
    "df_sliced = df_stretched.loc[start_date:end_date]\n",
    "\n",
    "plt.figure(figsize=(24, 3))\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['Gen.W_normalized'], label = 'Gen.W_normalized')\n",
    "sns.lineplot(x = df_sliced.index, y = df_sliced['stretched_theoretical'], label = 'stretched_theoretical')\n",
    "\n",
    "plt.title(f'[RECODED] Normalized Gen.W and Normalized and Stretched Theoretical Clear Sky Generation Over Time - With Azimuth = {mid_azimuth} and Tilt = {mid_tilt}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8. Getting Deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.1. Getting Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genW_resample['time'] = df_genW_resample.index\n",
    "df_stretched['delta-clear-gen'] = df_stretched['Gen.W_normalized'] - df_stretched['stretched_theoretical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MA = df_stretched.copy(deep=True)\n",
    "df_MA = df_MA.loc[:,['time','Gen.W','theoretical_clear-sky_generation.W','delta-clear-gen','Gen.W_normalized','stretched_theoretical']]\n",
    "df_MA = df_MA.resample('H').sum()\n",
    "df_MA['timestamp'] = df_MA.index\n",
    "df_MA = df_MA.rename(columns={\"delta-clear-gen\": \"value\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.2. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df_7_days['time'].min()\n",
    "end_date = df_7_days['time'].max()\n",
    "\n",
    "df_slice = df_MA[start_date:end_date]\n",
    "\n",
    "# Creating 2 subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 6), sharex=True) \n",
    "\n",
    "# plot 'Gen.W' and 'theoretical_clear-sky_generation.W' against 'time' in the first subplot\n",
    "sns.lineplot(ax=axes[0], x='timestamp', y='Gen.W_normalized', data=df_slice, label='Gen.W_normalized', color='blue')\n",
    "sns.lineplot(ax=axes[0], x='timestamp', y='stretched_theoretical', data=df_slice, label='stretched_theoretical', color='orange')\n",
    "axes[0].legend()\n",
    "\n",
    "# plot 'delta-clear-gen' against 'time' in the second subplot\n",
    "sns.lineplot(ax=axes[1], x='timestamp', y='value', data=df_slice, label='Delta Clear Gen', color='green')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()  # adjust subplot params so that the subplots fit into the figure area\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9. Identifying underperformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For plotting:\n",
    "start_date =  start_faults - timedelta(days=7)\n",
    "end_date =  end_faults + timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MA_inverted = df_MA.copy(deep=True)\n",
    "df_MA_inverted['value'] = df_MA['value'] * (-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.1. Adding information on Cloudy days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'timestamp' column in df_test to datetime if it's not\n",
    "df_MA_inverted['timestamp'] = pd.to_datetime(df_MA_inverted['timestamp'])\n",
    "\n",
    "# Create a new column in df_test with just the date (no time info)\n",
    "df_MA_inverted['date_only'] = df_MA_inverted['timestamp'].dt.date\n",
    "\n",
    "# Convert index 'time' to datetime and create a new column 'time' in df_site if it's not\n",
    "df_site['time'] = pd.to_datetime(df_site.index)\n",
    "\n",
    "# Create a new column in df_site with just the date (no time info)\n",
    "df_site['date_only'] = df_site['time'].dt.date\n",
    "\n",
    "# Merge df_site and df_test on the date_only column\n",
    "df_with_cloud = df_MA_inverted.merge(df_site[['date_only', 'is_low_cloudiness_day']], on='date_only', how='left')\n",
    "\n",
    "# If you want to drop the 'date_only' column after the merge:\n",
    "df_with_cloud = df_with_cloud.drop(columns=['date_only'])\n",
    "\n",
    "# Create a copy of 'timestamp' column\n",
    "df_with_cloud['timestamp_copy'] = df_with_cloud['timestamp']\n",
    "\n",
    "# Set 'timestamp' column as index\n",
    "df_with_cloud.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Rename the 'timestamp_copy' back to 'timestamp'\n",
    "df_with_cloud.rename(columns={'timestamp_copy': 'timestamp'}, inplace=True)\n",
    "\n",
    "df_MA_inverted = df_with_cloud.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot = df_MA_inverted[start_date:end_date]\n",
    "\n",
    "# Creating 2 subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 6), sharex=True) \n",
    "\n",
    "# plot 'Gen.W' and 'theoretical_clear-sky_generation.W' against 'time' in the first subplot\n",
    "sns.lineplot(ax=axes[0], x='timestamp', y='Gen.W_normalized', data=df_to_plot, label='Gen.W_normalized', color='blue')\n",
    "sns.lineplot(ax=axes[0], x='timestamp', y='stretched_theoretical', data=df_to_plot, label='stretched_theoretical', color='orange')\n",
    "axes[0].legend()\n",
    "\n",
    "# Separate data based on 'is_low_cloudiness_day'\n",
    "df_test_true = df_to_plot[df_to_plot['is_low_cloudiness_day'] == True]\n",
    "df_test_false = df_to_plot[df_to_plot['is_low_cloudiness_day'] == False]\n",
    "\n",
    "# plot 'delta-clear-gen' against 'time' in the second subplot for 'is_low_cloudiness_day' = True\n",
    "sns.lineplot(ax=axes[1], x='timestamp', y='value', data=df_test_true, label='Delta Clear Gen - Low Cloudiness', color='green')\n",
    "\n",
    "# plot 'delta-clear-gen' against 'time' in the second subplot for 'is_low_cloudiness_day' = False\n",
    "sns.lineplot(ax=axes[1], x='timestamp', y='value', data=df_test_false, label='Delta Clear Gen - High Cloudiness', color='gray')\n",
    "\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()  # adjust subplot params so that the subplots fit into the figure area\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.2. Limiting to a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_to_threshold(df, threshold_percentage=0.2):\n",
    "    # Make sure index is a DateTimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Get all unique dates in the df\n",
    "    unique_dates = df.index.normalize().unique()\n",
    "\n",
    "    # Iterate over each unique day in the df\n",
    "    for date in unique_dates:\n",
    "        # Convert to string for indexing\n",
    "        str_date = date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Get the max 'value' for the day\n",
    "        max_value = df.loc[str_date, 'stretched_theoretical'].max()\n",
    "\n",
    "        # Multiply the max 'value' by the threshold\n",
    "        threshold = max_value * threshold_percentage\n",
    "\n",
    "        # Find 'Gen.W' values that are below the threshold and replace them with 0\n",
    "        df.loc[(df.index.normalize() == date) & (df['value'] < threshold), 'value'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MA_inverted = limit_to_threshold(df_MA_inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot['value'].loc[start_date:end_date].plot(figsize=(24,6))\n",
    "\n",
    "# Creating 2 subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 6), sharex=True) \n",
    "\n",
    "# plot 'Gen.W' and 'theoretical_clear-sky_generation.W' against 'time' in the first subplot\n",
    "sns.lineplot(ax=axes[0], x='timestamp', y='Gen.W_normalized', data=df_to_plot, label='Gen.W_normalized', color='blue')\n",
    "sns.lineplot(ax=axes[0], x='timestamp', y='stretched_theoretical', data=df_to_plot, label='stretched_theoretical', color='orange')\n",
    "axes[0].legend()\n",
    "\n",
    "# Separate data based on 'is_low_cloudiness_day'\n",
    "df_test_true = df_to_plot[df_to_plot['is_low_cloudiness_day'] == True]\n",
    "df_test_false = df_to_plot[df_to_plot['is_low_cloudiness_day'] == False]\n",
    "\n",
    "# plot 'delta-clear-gen' against 'time' in the second subplot for 'is_low_cloudiness_day' = True\n",
    "sns.lineplot(ax=axes[1], x='timestamp', y='value', data=df_test_true, label='Delta Clear Gen - Low Cloudiness', color='green')\n",
    "\n",
    "# plot 'delta-clear-gen' against 'time' in the second subplot for 'is_low_cloudiness_day' = False\n",
    "sns.lineplot(ax=axes[1], x='timestamp', y='value', data=df_test_false, label='Delta Clear Gen - High Cloudiness', color='gray')\n",
    "\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()  # adjust subplot params so that the subplots fit into the figure area\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10. Identifying the recurrent faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10.1. Helper function to plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_date_range(df, start_date, end_date):\n",
    "    # Define timezone\n",
    "    tz = pytz.timezone(timezone_value)\n",
    "\n",
    "    # Converting string dates to datetime with timezone only if they are tz-naive\n",
    "    if start_date.tzinfo is None:\n",
    "        start_date = pd.to_datetime(start_date).tz_localize(tz)\n",
    "    if end_date.tzinfo is None:\n",
    "        end_date = pd.to_datetime(end_date).tz_localize(tz)\n",
    "\n",
    "    # Convert string dates to datetime with timezone\n",
    "    # start_date = pd.to_datetime(start_date).tz_localize(tz)\n",
    "   # end_date = pd.to_datetime(end_date).tz_localize(tz)\n",
    "    \n",
    "    # Slice the DataFrame based on the date range\n",
    "    mask = (df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)\n",
    "    df_sliced = df.loc[mask]\n",
    "\n",
    "    # Creating 3 subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(20, 9), sharex=True)\n",
    "\n",
    "    # plot 'Gen.W' and 'theoretical_clear-sky_generation.W' against 'time' in the new subplot\n",
    "    sns.lineplot(ax=axes[0], x='timestamp', y='Gen.W', data=df_sliced, label='Gen.W', color='blue')\n",
    "    sns.lineplot(ax=axes[0], x='timestamp', y='theoretical_clear-sky_generation.W', data=df_sliced, label='theoretical_clear-sky_generation.W', color='orange')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # plot 'transformed_Gen.W' and 'theoretical_clear-sky_generation.W' against 'time' in the second subplot\n",
    "    sns.lineplot(ax=axes[1], x='timestamp', y='Gen.W_normalized', data=df_sliced, label='Gen.W_normalized', color='blue')\n",
    "    sns.lineplot(ax=axes[1], x='timestamp', y='stretched_theoretical', data=df_sliced, label='stretched_theoretical', color='orange')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Separate sliced data based on 'is_low_cloudiness_day'\n",
    "    df_test_true = df_sliced[df_sliced['is_low_cloudiness_day'] == True]\n",
    "    df_test_false = df_sliced[df_sliced['is_low_cloudiness_day'] == False]\n",
    "\n",
    "    # plot 'delta-clear-gen' against 'time' in the third subplot for 'is_low_cloudiness_day' = True\n",
    "    sns.lineplot(ax=axes[2], x='timestamp', y='value', data=df_test_true, label='Delta Clear Gen - Low Cloudiness', color='green')\n",
    "\n",
    "    # plot 'delta-clear-gen' against 'time' in the third subplot for 'is_low_cloudiness_day' = False\n",
    "    sns.lineplot(ax=axes[2], x='timestamp', y='value', data=df_test_false, label='Delta Clear Gen - High Cloudiness', color='gray')\n",
    "\n",
    "    # plot 'underperformance' and 'is_recurrent_underperformance'\n",
    "    # plot gray 'X' for underperformance\n",
    "    axes[2].scatter(df_sliced[df_sliced['underperformance']]['timestamp'], df_sliced[df_sliced['underperformance']]['value'], color='gray', marker='x', label='Underperformance')\n",
    "\n",
    "    # plot red 'O' for is_recurrent_underperformance\n",
    "    axes[2].scatter(df_sliced[df_sliced['is_recurrent_underperformance']]['timestamp'], df_sliced[df_sliced['is_recurrent_underperformance']]['value'], color='red', marker='o', label='Recurrent Underperformance')\n",
    "\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()  # adjust subplot params so that the subplots fit into the figure area\n",
    "    return plt.gcf()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10.2. All positive records that happen on the same hour for consecutive days - With similar values - Skipping low-cloudiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_recurrent_underperformance(df, threshold_of_recurrence=3, value_threshold_percentage=20):\n",
    "    \n",
    "    if df.index.name != 'timestamp':\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    df['hour'] = df.index.hour\n",
    "    df['underperformance'] = df['value'] > 0  # Assuming 'value' > 0 means underperformance\n",
    "    df['is_recurrent_underperformance'] = False\n",
    "    df['count_for_hour_of_day'] = 0\n",
    "    \n",
    "    # Filter only rows where is_low_cloudiness_day is True\n",
    "    low_cloudiness_df = df[df['is_low_cloudiness_day'].fillna(False)]\n",
    "    \n",
    "    for hour in range(24):  # Looping through all hours of the day\n",
    "        hourly_df = low_cloudiness_df[low_cloudiness_df['hour'] == hour].copy()\n",
    "        \n",
    "        count = 0\n",
    "        potential_indices = []  # To keep track of the potential recurrent underperformances\n",
    "        \n",
    "        for idx, row in hourly_df.iterrows():\n",
    "            \n",
    "            if row['underperformance'] and row['value'] != 0:\n",
    "                \n",
    "                # For the first encounter with a fault\n",
    "                if not potential_indices:  # List is empty, initialize\n",
    "                    count = 1\n",
    "                    potential_indices = [idx]\n",
    "                    df.loc[idx, 'count_for_hour_of_day'] = count\n",
    "                    reference_value = row['value']\n",
    "                    \n",
    "                else:\n",
    "                    # Compute the percentage difference\n",
    "                    reference_value = hourly_df.loc[potential_indices[0], 'value']\n",
    "                    percentage_diff = abs((row['value'] - reference_value) / reference_value) * 100\n",
    "                    \n",
    "                    # Check if the value is within the threshold\n",
    "                    if percentage_diff <= value_threshold_percentage:\n",
    "                        count += 1\n",
    "                        df.loc[idx, 'count_for_hour_of_day'] = count\n",
    "                        potential_indices.append(idx)  # Store this index\n",
    "                    else:\n",
    "                        # Reset count and start tracking from the current index\n",
    "                        count = 1  \n",
    "                        df.loc[idx, 'count_for_hour_of_day'] = count\n",
    "                        potential_indices = [idx]  # Reset potential_indices list\n",
    "\n",
    "                # Check if count reaches the threshold\n",
    "                if count >= threshold_of_recurrence:\n",
    "                    # Set 'is_recurrent_underperformance' to True for all potential indices\n",
    "                    df.loc[potential_indices, 'is_recurrent_underperformance'] = True\n",
    "                \n",
    "            else:\n",
    "                # Reset count and clear list\n",
    "                count = 0  \n",
    "                potential_indices = []  \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_recurrent_underperformance(df_MA_inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10.2. Plotting results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot a certain range:\n",
    "plot_date_range(df_MA_inverted, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10.3. Saving plots to visual check all dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use the following to save ALL PLOTS from the date range.\n",
    "\n",
    "This takes up a lot of space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Getting first valid index\n",
    "index_start = df_MA_inverted.first_valid_index()\n",
    "# Getting last valid index\n",
    "index_end = df_MA_inverted.last_valid_index()\n",
    "\n",
    "# Create the desired directory if it doesn't exist\n",
    "dir_path = f\"./recurrent_faults_plots/{MID}\"\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "# start_plot = time_start - DateOffset(days=1)\n",
    "# end_plot = time_end + DateOffset(days=1)\n",
    "\n",
    "# Function to loop through the dates week by week and save plots\n",
    "def save_weekly_plots(time_start, time_end, df, plot_func, dir_path):\n",
    "    plt.ioff()  # Turn off interactive mode\n",
    "    current_start = time_start\n",
    "    while current_start < time_end:\n",
    "        current_end = current_start + pd.Timedelta(weeks=1)\n",
    "        \n",
    "        # Call the plot function\n",
    "        fig = plot_func(df, current_start, current_end)  # assume plot_func returns the figure\n",
    "        \n",
    "        # Save the plot\n",
    "        fig.savefig(f\"{dir_path}/{current_start.date()}_{current_end.date()}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        current_start = current_end\n",
    "    plt.ion()  # Turn on interactive mode\n",
    "\n",
    "\n",
    "# CAREFUL - THIS TAKES UP A LOT OF STORAGE:\n",
    "save_weekly_plots(index_start, index_end, df_MA_inverted, plot_date_range, dir_path)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Functions to label Level 1 faults based on raw signal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Generation Tripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_solar_tripping(df, trip_threshold=3):\n",
    "    # Ensure index is in the correct format\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['date_from_time'] = df['time'].dt.date\n",
    "    \n",
    "    # Create a boolean mask where generation is zero\n",
    "    df['is_zero_gen'] = df['Gen.W'] == 0\n",
    "\n",
    "    # Restrict to periods between sunrise and sunset\n",
    "    df['is_zero_gen'] = df['is_zero_gen'] & \\\n",
    "                                        (((df.index.hour * 60) + df.index.minute)>= df['sun_thre_start']) & (((df.index.hour * 60) + df.index.minute) <= df['sun_thre_end'])\n",
    "\n",
    "    # Identifying instances where generation goes to zero and then back up\n",
    "        ## First identify where is_zero_gen changes from False to True or True to False\n",
    "    df['diff_is_zero_gen'] = df['is_zero_gen'].astype(int).diff()\n",
    "\n",
    "        ## Then isolate instances where the generation is greater than 0\n",
    "    df['positive_gen'] = df['Gen.W'] > 0\n",
    "\n",
    "        ## Finally, combining both conditions\n",
    "    df['is_tripping'] = (df['diff_is_zero_gen'] != 0) & df['positive_gen']\n",
    "\n",
    "    # Get daily counts of tripping\n",
    "    daily_tripping_counts = df.resample('D')['is_tripping'].sum()\n",
    "\n",
    "    # Identify the dates where tripping counts cross the threshold\n",
    "    threshold_dates = daily_tripping_counts[daily_tripping_counts >= trip_threshold].index.date\n",
    "\n",
    "    # Convert df.index.date to a pandas Series\n",
    "    df_dates = pd.Series(df.index.date)\n",
    "    \n",
    "    # Update 'fault_tripping' column based on threshold_dates\n",
    "    df['fault_tripping'] = df['date_from_time'].isin(threshold_dates)\n",
    "\n",
    "    # I'm filling NaN values with False\n",
    "    df['fault_tripping'] = df['fault_tripping'].fillna(False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Non-zero generation tripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nonZero_tripping(df, threshold_nonZero_tripping=0.8):\n",
    "    # Initiatlising the column as False\n",
    "    df['nonZero_tripping'] = 0  \n",
    "    df['fault_nonZero_tripping'] = False  \n",
    "\n",
    "    # looping through and getting individual drops:\n",
    "    for i in range(1, len(df)):\n",
    "        # Get a ratio from measurements right after another\n",
    "        ratio = df['Gen.W'].iloc[i] / df['Gen.W'].iloc[i-1]\n",
    "\n",
    "        # If a drop is too significant, consider it a non-zero tripping:\n",
    "        if ratio < threshold_nonZero_tripping:\n",
    "            df['nonZero_tripping'].iloc[i] = 1\n",
    "\n",
    "    # Summing up all drops on a given day:\n",
    "    df['nonZero_tripping_sum'] = df['nonZero_tripping'].resample('D').transform('sum')\n",
    "\n",
    "    # Marking a day that this happens at least thrice:\n",
    "    df.loc[df['nonZero_tripping_sum'] >= 3, 'fault_nonZero_tripping'] = True\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Generation Clipping - Normalised with PVSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_solar_clipping(df, mid_pvsizewatt, threshold=0.001, min_duration='60min'):\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Calculate the rate of change of power\n",
    "    # Currently using diff, should use derivative?\n",
    "    df['power_diff_normalised'] = df['Gen.W'].diff()/mid_pvsizewatt\n",
    "\n",
    "    # We only want to check clipping in between the thresholds, therefore we'll need to define the hour:\n",
    "    # df['hour'] = df.index.hour\n",
    "\n",
    "    # Identifying periods where:\n",
    "    ##  the rate of change is near zero (potential clipping, considered by threshold), and \n",
    "    ## there more than 100 generation (sun_threshold), and\n",
    "    ## is between sun_thre_start and sun_thre_end\n",
    "    df['is_clipping'] = (np.abs(df['power_diff_normalised']) < threshold) & \\\n",
    "                        (df['Gen.W'] > 100) & \\\n",
    "                        (((df.index.hour * 60) + df.index.minute)>= df['sun_thre_start']) & (((df.index.hour * 60) + df.index.minute) <= df['sun_thre_end'])\n",
    "    \n",
    "    # Identify 'clipping periods', which are periods of clipping that last at least min_duration\n",
    "    ## First we get EVERY occurrence of clipping, for every 5 min\n",
    "    df['clipping_period'] = df['is_clipping'].diff().ne(0).cumsum()\n",
    "\n",
    "    # Then, we'll calculate the duration of each 'clipping period'\n",
    "    df['clipping_duration'] = df.groupby('clipping_period')['is_clipping'].transform('sum') * (df.index.to_series().diff().dt.total_seconds() / 60)\n",
    "\n",
    "    # If duration is longer than our min_duration, we label that as clipping\n",
    "    df['fault_clipping'] = df['is_clipping'] & (df['clipping_duration'] >= pd.Timedelta(min_duration).total_seconds() / 60)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Zero Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_zero_generation(df, min_duration='60min'):  \n",
    "    # Creating a boolean mask where generation is zero\n",
    "    # tbd -> optimise this with tripping\n",
    "    df['is_zero_gen'] = df['Gen.W'] == 0\n",
    "\n",
    "    # Restrict to periods between sunrise and sunset\n",
    "    df['is_zero_gen'] = df['is_zero_gen'] & (((df.index.hour * 60) + df.index.minute)>= df['sun_thre_start']) & (((df.index.hour * 60) + df.index.minute) <= df['sun_thre_end'])\n",
    "\n",
    "    '''   \n",
    "        # Trying with zero_gen and ghi not zero:\n",
    "        df['is_zero_gen'] = df['is_zero_gen'] & df['clear_sky_ghi'] > 0\n",
    "    '''\n",
    "\n",
    "    # Labeling periods of zero generation\n",
    "    df['zero_gen_period'] = df['is_zero_gen'].diff().ne(0).cumsum()\n",
    "\n",
    "    # Calculate the duration of each 'zero_gen_period'\n",
    "    df['zero_gen_duration'] = df.groupby('zero_gen_period')['is_zero_gen'].transform('sum') * (df.index.to_series().diff().dt.total_seconds() / 60)\n",
    "\n",
    "    # If duration is longer than our min_duration, we label that as zero_gen_period\n",
    "    df['fault_zero_gen'] = df['is_zero_gen'] & (df['zero_gen_duration'] >= pd.Timedelta(min_duration).total_seconds() / 60)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Recurring underperformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've already worked with df_MA_inverted, I'll just use it to label the original df_genW\n",
    "\n",
    "df_recurrent_und = df_MA_inverted.copy(deep=True)\n",
    "df_recurrent_und = df_recurrent_und[['timestamp','is_recurrent_underperformance']].rename(columns={'timestamp':'time'})\n",
    "\n",
    "# df_MA_inverted is a hourly dataframe, whereas df_genW has 5-minutes increments\n",
    "# df_MA_inverted['is_recurrent_underperformance'] is TRUE on an hourly basis:\n",
    "## If it happens to be TRUE on a full hour, I'll propagate it to it's corresponding 5-minutes increments:\n",
    "# Using forward fill to achieve this:\n",
    "\n",
    "df_recurrent_und = df_recurrent_und.resample('5T').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Night-time Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nightTime_gen(df):\n",
    "    # Create a boolean column for generation during night time\n",
    "    df['temp_fault_nightTime_gen'] = (df['Gen.W'] > 0) & \\\n",
    "                                     ~((((df.index.hour * 60) + df.index.minute) >= df['sun_thre_start']) & \\\n",
    "                                       (((df.index.hour * 60) + df.index.minute) <= df['sun_thre_end']))\n",
    "\n",
    "    # Create a new column to indicate if the nighttime generation occurred continuously for 1 hour (12 intervals)\n",
    "    df['fault_nightTime_gen_1hr'] = df['temp_fault_nightTime_gen'].rolling(window=12, min_periods=12).sum() == 12\n",
    "\n",
    "    # Drop the temporary column\n",
    "    df.drop(columns=['temp_fault_nightTime_gen'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Negative Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, need to use df_toCheck_negativeGen\n",
    "\n",
    "def detect_negative_gen(df, mid_pvsizewatt):\n",
    "\n",
    "    # Calculating the threshold (1% of mid_pvsizewatt)\n",
    "    threshold_neg = 0.01 * mid_pvsizewatt\n",
    "\n",
    "    # Creating a boolean to catch negative values\n",
    "    df['fault_negative_gen'] = (df['original_genW'] < 0) & (abs(df['original_genW']) >= threshold_neg)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8. Excessive Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_excessive_gen(df, mid_pvsizewatt):\n",
    "\n",
    "    # 100% on top of system size\n",
    "    threshold_excessive = 2 * mid_pvsizewatt\n",
    "\n",
    "    df['fault_excessive_gen'] = (df['Gen.W'] >= threshold_excessive)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9. No Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_no_data(df):\n",
    "    df['No Data'] = False\n",
    "    if df['Gen.W'].isna().all():\n",
    "        df['No Data'] = True\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Label all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dataframe(df):\n",
    "    # Extract hour from sunrise and sunset times\n",
    "\n",
    "    # Extract time from sunrise and sunset times\n",
    "    ## I was getting false positives since the sunset could be at 17:01, and by extracting the hour from the timestamp, I would get 17, and in that case I would have 12 possible slots to detect fault.\n",
    "    ## I'm doing a 1 hour buffer for the start, and flooring the sunset\n",
    "    df['index_minute_vale'] = ((df_genW.index.hour * 60) + df_genW.index.minute)\n",
    "    df['sun_thre_start'] = (((df['sunrise'].dt.hour + 1) % 24) * 60) + df['sunrise'].dt.minute\n",
    "    df['sun_thre_end'] = (df['sunset'].dt.hour * 60) + df['sunrise'].dt.minute\n",
    "    \n",
    "    df = detect_solar_tripping(df)\n",
    "    df = detect_nonZero_tripping(df)\n",
    "    df = detect_solar_clipping(df, mid_pvsizewatt)\n",
    "    df = detect_zero_generation(df)\n",
    "    df = detect_negative_gen(df, mid_pvsizewatt)\n",
    "    df = detect_nightTime_gen(df)\n",
    "    df = detect_excessive_gen(df, mid_pvsizewatt)\n",
    "    df = detect_no_data(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labelled = label_dataframe(df_genW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll have to resample the df_MA_inverted to 5 minutes intervals\n",
    "# The only thing I really care about here is the 'is_recurrent_underperformance' column, so I'll use forward fill:\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.ffill.html\n",
    "\n",
    "\n",
    "df_labelled['time'] = pd.to_datetime(df_labelled['time'])\n",
    "df_MA_inverted.index = pd.to_datetime(df_MA_inverted.index)\n",
    "\n",
    "df_recurrent_und = df_MA_inverted.copy(deep=True)\n",
    "df_recurrent_und = df_recurrent_und[['timestamp','is_recurrent_underperformance', 'is_low_cloudiness_day']].rename(columns={'timestamp':'time'})\n",
    "\n",
    "# Resample the df_MA_inverted to 5-minute intervals using forward filling\n",
    "df_recurrent_und = df_recurrent_und.resample('5T').ffill()\n",
    "\n",
    "# Merge the dataframes on the timestamp columns\n",
    "merged_df = pd.merge(df_labelled, df_recurrent_und[['is_recurrent_underperformance', 'is_low_cloudiness_day']], left_on='time', right_index=True, how='left')\n",
    "\n",
    "# Rename the is_recurrent_underperformance column\n",
    "merged_df.rename(columns={'is_recurrent_underperformance': 'fault_recurrent_underperformance'}, inplace=True)\n",
    "\n",
    "# Now merged_df contains the df_genW data with the added is_recurrent_underperformance column\n",
    "df_labelled = merged_df\n",
    "\n",
    "# filling any NaN values in the new column with a default value (e.g., False)\n",
    "df_labelled['fault_recurrent_underperformance'].fillna(False, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faults(row):\n",
    "    faults = []\n",
    "    if row['fault_tripping']:\n",
    "        faults.append('fault_tripping')\n",
    "    if row['fault_nonZero_tripping']:\n",
    "        faults.append('fault_nonZero_tripping')\n",
    "    if row['fault_clipping']:\n",
    "        faults.append('fault_clipping')\n",
    "    if row['fault_zero_gen']:\n",
    "        faults.append('fault_zero_gen')\n",
    "    if row['fault_recurrent_underperformance']:\n",
    "        faults.append('fault_recurrent_underperformance')\n",
    "    if row['fault_negative_gen']:\n",
    "        faults.append('Negative Generation')\n",
    "    if row['fault_nightTime_gen_1hr']:\n",
    "        faults.append('Night-Time Generation')\n",
    "    if row['fault_excessive_gen']:\n",
    "        faults.append('Excessive Generation')\n",
    "    if row['No Data']:\n",
    "        faults.append('No Data')\n",
    "    return faults\n",
    "\n",
    "df_labelled['faults'] = df_labelled.apply(find_faults, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Saving individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save it:\n",
    "df_labelled.to_csv(f'./2A_individual_outputs/{MID}.csv')\n",
    "\n",
    "print(f'Fault Detection saved for {MID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_day = df_labelled.copy()\n",
    "\n",
    "df_per_day.index = df_per_day.index.date\n",
    "\n",
    "def agg_faults_per_day(faults_list):\n",
    "    unique_faults_on_that_day = list(set([fault for sublist in faults_list for fault in sublist]))\n",
    "    return unique_faults_on_that_day\n",
    "\n",
    "result_per_day = df_per_day.groupby(df_per_day.index).agg({'Gen.W': 'sum', 'faults': agg_faults_per_day})\n",
    "\n",
    "result_per_day.to_csv(f'./2A_individual_outputs/per_day/{MID}_per_day.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# List of possible faults\n",
    "faults_list = ['fault_tripping', 'fault_clipping', 'fault_zero_gen', 'fault_recurrent_underperformance']\n",
    "\n",
    "# Assuming df_labelled is your DataFrame\n",
    "# If 'timestamp' is not the index, set it\n",
    "df_labelled.index = pd.to_datetime(df_labelled.index)\n",
    "\n",
    "# Resample DataFrame by weeks\n",
    "weeks = [group for _, group in df_labelled.resample('W-MON')]\n",
    "\n",
    "# Determine the number of rows for the subplots (one week per row)\n",
    "nrows = len(weeks)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, figsize=[15, 5 * nrows])\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Make sure axes is always a 1D array, even if there's only one subplot\n",
    "if nrows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, week in enumerate(weeks):\n",
    "    ax = axes[i]\n",
    "    ax.set_ylabel('Gen.W')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_title(f'Gen.W over Time for Week {i+1}')\n",
    "\n",
    "    # Iterate through the days of the week\n",
    "    days = [group for _, group in week.resample('D')]\n",
    "    for day in days:\n",
    "        color = 'blue' # Default color for no faults\n",
    "        label = None # Default label\n",
    "\n",
    "        # Check for faults in the day\n",
    "        for fault in faults_list:\n",
    "            if any(fault in faults for faults in day['faults']):\n",
    "                color = 'red' # Change color if there's a fault\n",
    "                label = 'Fault' # Set the label for the legend\n",
    "                break\n",
    "\n",
    "        ax.plot(day['Gen.W'], color=color, label=label)\n",
    "\n",
    "    # Set the x-tick labels to be vertical\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Add legend without duplicate labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# List of possible faults\n",
    "faults_list = ['fault_tripping', 'fault_clipping', 'fault_zero_gen', 'fault_recurrent_underperformance']\n",
    "\n",
    "# Colors for different cases\n",
    "colors = {\n",
    "    'no_monitor_fault': 'green',\n",
    "    'low_cloudiness': 'grey'\n",
    "}\n",
    "colors.update({fault: 'red' for fault in faults_list})\n",
    "\n",
    "# Assuming df_labelled is your DataFrame\n",
    "# If 'timestamp' is not the index, set it\n",
    "df_labelled.index = pd.to_datetime(df_labelled.index)\n",
    "\n",
    "# Resample DataFrame by weeks\n",
    "weeks = [group for _, group in df_labelled.resample('W-MON')]\n",
    "\n",
    "# Determine the number of rows for the subplots (one week per row)\n",
    "nrows = len(weeks)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, figsize=[15, 5 * nrows])\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Make sure axes is always a 1D array, even if there's only one subplot\n",
    "if nrows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, week in enumerate(weeks):\n",
    "    ax = axes[i]\n",
    "    ax.set_ylabel('Gen.W')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_title(f'Gen.W over Time for Week {i+1}')\n",
    "\n",
    "    # Iterate through the days of the week\n",
    "    days = [group for _, group in week.resample('D')]\n",
    "    for day in days:\n",
    "        label = 'no_monitor_fault' # Default label for no monitor faults\n",
    "        if all(day['is_low_cloudiness_day'] == False):\n",
    "            label = 'low_cloudiness' # Label for low cloudiness\n",
    "\n",
    "        # Check for faults in the day\n",
    "        for fault in faults_list:\n",
    "            if any(fault in faults for faults in day['faults']):\n",
    "                label = fault # Set label to the specific fault\n",
    "                break\n",
    "\n",
    "        ax.plot(day['Gen.W'], color=colors[label], label=label)\n",
    "\n",
    "    # Set the x-tick labels to be vertical\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Add legend without duplicate labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "plt.show()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
