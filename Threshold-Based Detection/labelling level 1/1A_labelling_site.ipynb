{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Performance Analysis\n",
    "\n",
    "This document runs a single site through a performance ratio analsys.\n",
    "\n",
    "It uses this analyses to label the site (on that day) based on Level 1 faults"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Libraries import\n",
    "# ========================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import pytz\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = AWS Credentials\n",
    "# ========================================================\n",
    "\n",
    "PROD_AWS_PROFILE = \"gsesami-prod\"\n",
    "AWS_REGION = \"ap-southeast-2\"\n",
    "\n",
    "prod_session = boto3.session.Session(profile_name=PROD_AWS_PROFILE)\n",
    "\n",
    "prod_client = prod_session.client(\n",
    "    \"timestream-query\", region_name=AWS_REGION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining the Site ID, and dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all sites\n",
    "sites_list = pd.read_csv('./input_data/Site_List.csv')\n",
    "\n",
    "# Reading all monitors\n",
    "monitors_list = pd.read_csv('./input_data/Monitors_List.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define site_id to be analysed:\n",
    "site_id = 'c4e9450c-1d4c-44ad-af9e-38a0b4a1a58d'\n",
    "\n",
    "# Time period\n",
    "date_start = '2023-04-25'   \n",
    "date_end = '2023-10-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting site_id name:\n",
    "site_id_full = 'SITE|' + str(site_id)\n",
    "\n",
    "# Checking\n",
    "print(\"This analysis will be performed on the site: \", site_id_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking timezone\n",
    "timezone_value = 'Australia/Sydney'\n",
    "timezone_value = sites_list[sites_list['source'] == site_id_full].iloc[0]['timezone']\n",
    "\n",
    "time_starttz = pytz.timezone('UTC').localize(datetime.datetime.strptime(date_start, '%Y-%m-%d'))\n",
    "time_endtz = pytz.timezone('UTC').localize(datetime.datetime.strptime(date_end, '%Y-%m-%d'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Querying and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metric_site(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(timeid, measure_name, data_values):\n",
    "    # ============== Check if there is data available for the pv system =============\n",
    "    if len(timeid)!=0:\n",
    "        timeid = pd.to_datetime(timeid)\n",
    "        if timeid.tzinfo is None:\n",
    "            print('this is not tz-aware')\n",
    "            if timezone_value is not None:\n",
    "                timeid = timeid.tz_localize('UTC').tz_convert(timezone_value)\n",
    "                # timeid = timeid.tz_localize(timezone_list[i])\n",
    "            else:\n",
    "                print('no timezone in the table')\n",
    "                timeid = timeid.tz_localize('UTC').tz_convert('Australia/Sydney')\n",
    "                # timeid = timeid.tz_localize('Australia/Sydney')\n",
    "        else:\n",
    "            print('this is tz-aware')\n",
    "        \n",
    "        timesort = timeid.sort_values()\n",
    "        data = pd.DataFrame(data={'time':timeid, measure_name: data_values})\n",
    "        data.sort_values('time', inplace=True)\n",
    "        data.set_index('time', inplace=True)\n",
    "        data[measure_name] = data[measure_name].astype(float)\n",
    "    else:\n",
    "        data = pd.DataFrame(data_values, index=timeid, columns=[measure_name])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# = Merging clear skies and expected\n",
    "# ==================================\n",
    "\n",
    "def merge_clear_expe(df1, df2):\n",
    "    df_merged = df1.join(df2)\n",
    "    df_merged['expected_over_clear'] =  (df_merged['Irrad.kWh.m2.Daily'] / df_merged['EnergyYield.kWh.Daily'] * 100).round(0)\n",
    "    df_merged['date'] =  df_merged.index\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# = Getting colours from values for plotting bars\n",
    "# ===============================================\n",
    "\n",
    "def colors_from_values(values, palette_name):\n",
    "    # normalize the values to range [0, 1]\n",
    "    normalized = (values - min(values)) / (max(values) - min(values))\n",
    "    # convert to indices\n",
    "    indices = np.round(normalized * (len(values) - 1)).astype(np.int32)\n",
    "    # use the indices to get the colors\n",
    "    palette = sns.color_palette(palette_name, len(values))\n",
    "    return np.array(palette).take(indices, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Getting values Getting the clear sky values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Clear sky values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# = Reading EnergyYield.kWh.Daily from AWS TimeStream\n",
    "# ===================================================\n",
    "\n",
    "measure_name = 'EnergyYield.kWh.Daily'\n",
    "timeid, data_values = read_metric_site(date_start, date_end, measure_name, site_id)\n",
    "df_clear = build_dataframe(timeid, measure_name, data_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Expected Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# = Reading Irrad.kWh.m2.Daily from AWS TimeStream\n",
    "# ================================================\n",
    "\n",
    "measure_name = 'Irrad.kWh.m2.Daily'\n",
    "timeid, data_values = read_metric_site(date_start, date_end, measure_name, site_id)\n",
    "df_expected = build_dataframe(timeid, measure_name, data_values)\n",
    "# Fixing it as a float:\n",
    "df_expected['Irrad.kWh.m2.Daily'] = df_expected['Irrad.kWh.m2.Daily'].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Measured Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# = Reading Production.kWh.Daily from AWS TimeStream\n",
    "# ==================================================\n",
    "\n",
    "measure_name = 'Production.kWh.Daily'\n",
    "timeid, data_values = read_metric_site(date_start, date_end, measure_name, site_id)\n",
    "df_production = build_dataframe(timeid, measure_name, data_values)\n",
    "# Fixing it as float\n",
    "df_production['Production.kWh.Daily'] = df_production['Production.kWh.Daily'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging clear and expected:\n",
    "df_merged = df_clear.join(df_expected)\n",
    "# Merging (clear and expected) and production\n",
    "df_merged = df_merged.join(df_production)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Cloudiness - Comparison on Clear skies x Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the performance ratio:\n",
    "df_merged['expected_over_clear'] =  (df_merged['Irrad.kWh.m2.Daily'] / df_merged['EnergyYield.kWh.Daily'] * 100).round(0)\n",
    "# Getting this extra column flor plotting:\n",
    "df_merged['date'] =  df_merged.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Adding low-cloudiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Checking values above a certain threshold when comparing clear skies and expected\n",
    "# ========================================================\n",
    "\n",
    "# Define the threshold for low cloudiness days:\n",
    "threshold_low_cloudiness = 80\n",
    "\n",
    "# Make it low_cloudiness aware:\n",
    "df_merged.loc[df_merged['expected_over_clear'] >= threshold_low_cloudiness, 'is_low_clousdiness_day'] = True \n",
    "df_merged.loc[df_merged['expected_over_clear'] < threshold_low_cloudiness, 'is_low_clousdiness_day'] = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Daily performance ratio (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting new df to setup following analysis:\n",
    "df_performance = df_merged\n",
    "df_performance['Performance.perc.Daily'] = (df_performance['Production.kWh.Daily'] / df_performance['Irrad.kWh.m2.Daily'] * 100).round(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Weekend and Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the day of the week using the weekday() method from dataframe\n",
    "df_performance['day_of_week'] = df_performance['date'].apply(lambda x: x.weekday())\n",
    "\n",
    "# create a binary indicator variable for weekends vs weekdays\n",
    "df_performance['is_weekend'] = df_performance['day_of_week'].apply(lambda x: x in [5, 6])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the month using the datetime.month attribute\n",
    "df_performance['month'] = df_performance['date'].dt.month\n",
    "\n",
    "# Dictionary to map the month to the season\n",
    "# Note that this has been done for Australia (Southern hemisphere)\n",
    "\n",
    "seasons = {1: 'summer', \n",
    "           2: 'summer', \n",
    "           3: 'autumn', \n",
    "           4: 'autumn', \n",
    "           5: 'autumn', \n",
    "           6: 'winter', \n",
    "           7: 'winter', \n",
    "           8: 'winter', \n",
    "           9: 'spring', \n",
    "           10: 'spring', \n",
    "           11: 'spring', \n",
    "           12: 'summer'\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "df_performance['season'] = df_performance['month'].apply(lambda x: seasons[x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Kicking off outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary to kick off outliers:\n",
    "#df_performance = df_performance[df_performance['Performance.perc.Daily'] < 120]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Exploring the performance variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Functions to check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_check(row):\n",
    "    if row['Performance.perc.Daily'] >= 80:\n",
    "        val = 'ok'\n",
    "    elif row['Performance.perc.Daily'] >=60:\n",
    "        val = 'medium'\n",
    "    else:\n",
    "        val = 'under'\n",
    "    return val\n",
    "\n",
    "def performance_and_LC_check(row):\n",
    "    if row['is_low_clousdiness_day'] == False:\n",
    "        val = 'High Cloudiness'\n",
    "    else:\n",
    "        val = row['performancelabel']\n",
    "    return val\n",
    "\n",
    "df_performance['performancelabel'] = df_performance.apply(performance_check, axis=1)\n",
    "df_performance['performancelabel'] = df_performance.apply(performance_and_LC_check, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Visual performance check - Barplots (Expected over measured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette ={\"ok\": \"green\", \"medium\": \"yellow\", \"under\": \"red\", \"High Cloudiness\":\"grey\"}\n",
    "\n",
    "matplotlib.rc_file_defaults()\n",
    "\n",
    "ax1 = sns.set_style(style=None, rc=None)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(50,15))\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "\n",
    "sns.lineplot(data = df_performance['Irrad.kWh.m2.Daily'], marker='o', sort = False, ax=ax1, label='Expected', color='green')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "sns.lineplot(data = df_performance['Production.kWh.Daily'], marker='X', sort = False, ax=ax1, label='Measured', color='blue')\n",
    "\n",
    "sns.barplot(data = df_performance, x='date', y='Performance.perc.Daily', hue='performancelabel', palette=palette, alpha=0.8, dodge=None)\n",
    "\n",
    "fig.suptitle('Site Name = '+ str(site_id_full) +'\\nSite ID = '+ str(site_id_full) +'\\nPerformance over time (daily aggregate) - All days' + '\\nLow cloudiness threshold = '+ str(threshold_low_cloudiness) + '% [expected/clear_sky]')\n",
    "sns.move_legend(ax1, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "sns.move_legend(ax2, \"upper left\", bbox_to_anchor=(1, 0.8))\n",
    "\n",
    "figname = str(site_id + '.png')\n",
    "\n",
    "## Saving:\n",
    "# fig.savefig(figname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Working with low cloudiness days"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Getting low cloudiness days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LC = df_performance[df_performance['is_low_clousdiness_day'] == True]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Visual check on low cloudiness' days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette ={\"ok\": \"green\", \"medium\": \"yellow\", \"under\": \"red\"}\n",
    "\n",
    "matplotlib.rc_file_defaults()\n",
    "ax1 = sns.set_style(style=None, rc=None )\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(40,10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "\n",
    "fig.suptitle('Site Name = '+ str(site_id_full) +'\\nSite ID = '+ str(site_id_full) +'\\nPerformance over time (daily aggregate) - Only low cloudiness days' + '\\nLow cloudiness threshold = '+ str(threshold_low_cloudiness) + '% [expected/clear_sky]')\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=df_LC, \n",
    "    x='date',\n",
    "    y='Performance.perc.Daily',\n",
    "    hue='performancelabel',\n",
    "    palette=palette,\n",
    "    dodge=None\n",
    "    )\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Functions to label Level 1 faults based on performance ratio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Major Underperformance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System is performing at less than 60 % for 3 days or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_major_underperformance(df, window_size, threshold):\n",
    "    # create a boolean mask to identify the rows where the 'Performance.perc.Daily' column is below 60 (or threshold)\n",
    "    # we'll use this to filter out the dataframe\n",
    "    mask = df['Performance.perc.Daily'] < threshold\n",
    "\n",
    "    # create a new column 'majpr_underperformance' with the default value 'FALSE'\n",
    "    df['major_underperformance'] = 'FALSE'\n",
    "\n",
    "    # label the rows where the 'Performance.perc.Daily' column has dropped below 60 for 3 consecutive days or more\n",
    "    # 1st, loop through the df:\n",
    "    for i in range((window_size-1), len(df)):\n",
    "        # check if the mask apply for N consecutive days (window_size):\n",
    "        if all(mask.iloc[i - j] for j in range(window_size)):\n",
    "            # Label it as such:\n",
    "            # Mark all days in the sequence as 'Minor underperformance'\n",
    "            ## Using window_size to do this retroactively:\n",
    "            for j in range(window_size):\n",
    "                df.loc[df.index[i-j], 'major_underperformance'] = 'Major underperformance'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Minor Underperformance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System is performing at less than 80 % for 7 days or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_minor_underperformance(df, window_size, threshold):\n",
    "    # create a boolean mask to identify the rows where the 'Performance.perc.Daily' column is below the threshold\n",
    "    mask = df['Performance.perc.Daily'] < threshold\n",
    "\n",
    "    # create a new column 'minor_underperformance' with the default value 'FALSE'\n",
    "    df['minor_underperformance'] = 'FALSE'\n",
    "\n",
    "    # label the rows where the 'Performance.perc.Daily' column has dropped below the threshold for the specified number of consecutive days or more\n",
    "    # 1st, loop through the df:\n",
    "    for i in range((window_size-1), len(df)):\n",
    "        # check if the mask apply for N consecutive days (window_size):        \n",
    "        if mask.iloc[i] and all(mask.iloc[i-window_size+1:i+1]):\n",
    "            # Label it as such:\n",
    "            # Mark all days in the sequence as 'Minor underperformance'\n",
    "            ## Using window_size to do this retroactively:\n",
    "            for j in range(window_size):\n",
    "                df.loc[df.index[i-j], 'minor_underperformance'] = 'Minor underperformance'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Weekend and Weekdays underperformance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekend Underperformance: The performance is often lower on weekends.\n",
    "\n",
    "Weekdays Underperformance: The performance is often lower on weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if:\n",
    "# The performance of days in the current week (weekdays or weekends) \n",
    "# falls below the performance of the opposite type of days (weekends or weekdays) \n",
    "# from the previous week by a certain threshold. \n",
    "# If so, it labels them as underperforming. \n",
    "\n",
    "\n",
    "def check_week_performance(df, threshold, performance_col='Performance.perc.Daily'):\n",
    "    # create new columns for week and week performance labels\n",
    "    # using datetime to add week\n",
    "    df['Week'] = df.index.to_period('W').astype(str)\n",
    "    # initialising weekday average\n",
    "    df['Prev Weekday Avg'] = 0\n",
    "    # initialising weekend average\n",
    "    df['Prev Weekend Avg'] = 0\n",
    "    # initialising default week performance value (FALSE)\n",
    "    # this might be changed to either 'weekday underperformance' or 'weekend underperformance' (or remain unchanged)\n",
    "    df['week_underperformance'] = 'FALSE'\n",
    "\n",
    "    # calculating the weekday and weekend averages for each week\n",
    "    unique_weeks = df['Week'].unique()\n",
    "    for i, week in enumerate(unique_weeks):\n",
    "        if i == 0:  # skip the first week as there is no previous week to compare\n",
    "            continue\n",
    "\n",
    "        prev_week = unique_weeks[i - 1]\n",
    "        prev_week_df = df[df['Week'] == prev_week]\n",
    "\n",
    "        # for weekdays:\n",
    "        prev_weekday_avg = prev_week_df.loc[~prev_week_df['is_weekend'], performance_col].mean()\n",
    "        # for weekends:\n",
    "        prev_weekend_avg = prev_week_df.loc[prev_week_df['is_weekend'], performance_col].mean()\n",
    "\n",
    "        # label the rows where the performance is below the average for their respective day types against the previous week's daytypes\n",
    "\n",
    "        # For each day in the current week, the function checks:\n",
    "        ## If it's a weekday and its performance is below the previous week's weekend average minus the threshold, it labels it as 'Weekday underperformance'\n",
    "        ## If it's a weekend and its performance is below the previous week's weekday average minus the threshold, it labels it as 'Weekend underperformance'.\n",
    "        df.loc[(df['Week'] == week) & (~df['is_weekend']) & (df[performance_col] < prev_weekend_avg - threshold), 'week_underperformance'] = 'Weekday underperformance'\n",
    "        df.loc[(df['Week'] == week) & (df['is_weekend']) & (df[performance_col] < prev_weekday_avg - threshold), 'week_underperformance'] = 'Weekend underperformance'\n",
    "\n",
    "        # store the previous week's averages in the dataframe\n",
    "        df.loc[df['Week'] == week, 'Prev Weekday Avg'] = prev_weekday_avg\n",
    "        df.loc[df['Week'] == week, 'Prev Weekend Avg'] = prev_weekend_avg\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. Seasonal underperformance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter Underperformance: There is a seasonal underperformance in winter, when compared to the average of the other 3 seasons.\n",
    "\n",
    "Summer Underperformance: There is a seasonal underperformance in summer, when compared to the average of the other 3 seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_seasonal_performance(df, threshold):\n",
    "    # Initialize the 'seasonal_fault' column with False\n",
    "    df['seasonal_underperformance'] = 'FALSE'\n",
    "    \n",
    "    # We want to check Seasonal underperfromance only after we have 1 year worth of data.\n",
    "    # We'll get check if the dataset has at leat 1 year worth of data:\n",
    "    ## Get the date of the oldest record in the dataset\n",
    "    oldest_date = df.index.min()\n",
    "    ## Get the current date and make it timezone-aware\n",
    "    current_date = df.index.min()\n",
    "    # Check if the oldest record is more than a year old\n",
    "    if (current_date - oldest_date) < timedelta(days=365):\n",
    "        print(\"Oldest data is less than a year old. The function needs data older than one year.\")\n",
    "        return df\n",
    "    \n",
    "    # Get the average for each season\n",
    "    season_avg = df.groupby('season')['Performance.perc.Daily'].mean()\n",
    "    \n",
    "    # Create a new column 'season_avg_performance' with the average value for the corresponding season\n",
    "    df['season_avg_performance'] = df['season'].apply(lambda x: season_avg[x])\n",
    "    \n",
    "    # Calculate the sum of the averages for the other seasons\n",
    "    other_season_sum = {\n",
    "        'summer': (season_avg['autumn'] + season_avg['winter'] + season_avg['spring']) / 3,\n",
    "        'winter': (season_avg['autumn'] + season_avg['summer'] + season_avg['spring']) / 3\n",
    "    }\n",
    "    \n",
    "    # Check for underperformance and update the 'seasonal_fault' column accordingly\n",
    "    for index, row in df.iterrows():\n",
    "        season = row['season']\n",
    "        performance = row['Performance.perc.Daily']\n",
    "\n",
    "        # Only start labeling after a year has passed\n",
    "        if (index - oldest_date) < timedelta(days=365):\n",
    "            continue\n",
    "\n",
    "        if season == 'summer' and performance < other_season_sum['summer'] - threshold:\n",
    "            df.loc[index, 'seasonal_underperformance'] = 'summer underperformance'\n",
    "        elif season == 'winter' and performance < other_season_sum['winter'] - threshold:\n",
    "            df.loc[index, 'seasonal_underperformance'] = 'winter underperformance'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Labelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for checking major underperformance\n",
    "window_size_major_und = 3\n",
    "threshold_performance_major_und = 60\n",
    "\n",
    "# Parameters for checking minor underperformance\n",
    "window_size_minor_und = 7\n",
    "threshold_performance_minor_und = 80\n",
    "\n",
    "# Parameters for weekend and weekdays underperormance\n",
    "threshold_performance_weekends_weekdays = 20\n",
    "\n",
    "# Parameters for checking seasonal underperformance\n",
    "threshold_performance_seasonal = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. Running functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_major_underperformance(df_LC, window_size_major_und, threshold_performance_major_und)\n",
    "\n",
    "check_minor_underperformance(df_LC, window_size_minor_und, threshold_performance_minor_und)\n",
    "\n",
    "check_week_performance(df_LC, threshold_performance_weekends_weekdays)\n",
    "\n",
    "check_seasonal_performance(df_LC, threshold_performance_seasonal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the performance and label columns\n",
    "level1_sites_labels_df = df_LC[[\n",
    "    'EnergyYield.kWh.Daily','Irrad.kWh.m2.Daily','Production.kWh.Daily','Performance.perc.Daily',\n",
    "    'major_underperformance', 'minor_underperformance', 'week_underperformance', 'seasonal_underperformance'\n",
    "    ]]\n",
    "\n",
    "# Function to create the array for the new column\n",
    "# It starts from 4:, since the first 4 columns are NOT meant to be in the level1-labels-site columns\n",
    "def create_array(row):\n",
    "    return [value for value in row[4:] if value != 'FALSE']\n",
    "\n",
    "# Create the new 'level1-labels-site' column\n",
    "level1_sites_labels_df['level1-labels-site'] = level1_sites_labels_df.apply(create_array, axis=1)\n",
    "\n",
    "\n",
    "# If want to drop rows with empty arrays in 'level1-labels-site':\n",
    "# Currently not dropping, to keep rows in which there wasn't any fault.\n",
    "# level1_sites_labels_df = level1_sites_labels_df.loc[level1_sites_labels_df['level1-labels-site'].apply(len) > 0]\n",
    "\n",
    "# Keeping even the empty arrays to showcase non-faulty days:\n",
    "level1_sites_labels_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save it:\n",
    "output_dir = './1A_individual_outputs/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "level1_sites_labels_df.to_csv(f'{output_dir}{site_id}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdcc8738350348f5da3cd958a6f31e3755dac4740b3163f526dc5114836e977f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
