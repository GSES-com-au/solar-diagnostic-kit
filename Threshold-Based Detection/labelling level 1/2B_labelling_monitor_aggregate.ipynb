{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Libraries import\n",
    "# ========================================================\n",
    "\n",
    "# from collections import defaultdict\n",
    "import numpy as np\n",
    "import boto3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "# from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "#import os\n",
    "\n",
    "#from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "\n",
    "#import math\n",
    "\n",
    "import pvlib\n",
    "from pvlib import irradiance\n",
    "#from pvlib import location\n",
    "\n",
    "# import pickle # dump variables\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import plotly.express as px\n",
    "#import plotly.graph_objects as go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = AWS Credentials\n",
    "# ========================================================\n",
    "\n",
    "PROD_AWS_PROFILE = \"gsesami-prod\"\n",
    "AWS_REGION = \"ap-southeast-2\"\n",
    "\n",
    "prod_session = boto3.session.Session(profile_name=PROD_AWS_PROFILE)\n",
    "\n",
    "prod_client = prod_session.client(\n",
    "    \"timestream-query\", region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time period\n",
    "time_start = '2020-01-01'\n",
    "\n",
    "# Setting date_end to today\n",
    "today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "time_end = today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Thresholds\n",
    "# ========================================================\n",
    "\n",
    "# Cloudiness\n",
    "# Define the threshold for low cloudiness days:\n",
    "threshold_low_cloudiness = 80"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Querying TimeStream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Monitor ID, Site ID, and Labelled Faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all sites\n",
    "sites_list = pd.read_csv('./input_data/Site_List.csv')\n",
    "# Reading all monitors\n",
    "monitors_list = pd.read_csv('./input_data/Monitors_List.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read from the labelled dataset\n",
    "def get_site_id_full(df, sequence):\n",
    "    site_id_full = df['siteId'].loc[sequence]\n",
    "    return site_id_full\n",
    "\n",
    "def get_site_id(site_id_full):\n",
    "    site_id = site_id_full.removeprefix('SITE|')\n",
    "    return site_id\n",
    "\n",
    "def get_mid_full(df, sequence):\n",
    "    MID_full = df['source'].loc[sequence]\n",
    "    return MID_full\n",
    "\n",
    "def get_mid_id(MID_full):\n",
    "    MID = MID_full.removeprefix('MNTR|')\n",
    "    return MID\n",
    "\n",
    "# =======================\n",
    "# = Aggregate Function\n",
    "# =======================\n",
    "\n",
    "def get_mid_info(df, sequence):\n",
    "    site_id_full = get_site_id_full(df, sequence)\n",
    "    site_id = get_site_id(site_id_full)\n",
    "    MID_full = get_mid_full(df, sequence)\n",
    "    MID = get_mid_id(MID_full)\n",
    "\n",
    "    return site_id_full, site_id, MID_full, MID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_id(site_id_full):\n",
    "    site_id = site_id_full.removeprefix('SITE|')\n",
    "    return site_id\n",
    "\n",
    "# =======================\n",
    "# = Aggregate Function\n",
    "# =======================\n",
    "\n",
    "def get_site_info(df, sequence):\n",
    "    site_id_full = get_site_full(df, sequence)\n",
    "    site_id = get_site_id(site_id_full)\n",
    "\n",
    "    return site_id_full, site_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Getting Timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timezones(site_id_full, sites_list, time_start, time_end):\n",
    "    # Checking timezone, using Sydney as a backup\n",
    "    timezone_value = 'Australia/Sydney'\n",
    "    timezone_value = sites_list[sites_list['source'] == site_id_full].iloc[0]['timezone']\n",
    "\n",
    "    time_starttz = pytz.timezone('UTC').localize(datetime.datetime.strptime(time_start, '%Y-%m-%d'))\n",
    "    time_endtz = pytz.timezone('UTC').localize(datetime.datetime.strptime(time_end, '%Y-%m-%d'))\n",
    "\n",
    "    print(f'This monitor is located at: {timezone_value}')\n",
    "    \n",
    "    return timezone_value, time_starttz, time_endtz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Getting Monitor Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_monitor_metadata(monitors_list):\n",
    "\n",
    "    print('-- Fetching Monitor Metadata')\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Making sure the monitor list is of type string:\n",
    "    monitors_list['source'] = monitors_list['source'].astype(str)\n",
    "\n",
    "    # filtering the dataframe based on the monitor in question:\n",
    "    monitor_row = monitors_list.loc[monitors_list['source'] == MID_full]\n",
    "\n",
    "    # If the monitor ID exists, isolate each useful variable:\n",
    "    if not monitor_row.empty:\n",
    "        # latitude\n",
    "        ## Notice that the data output here for latitude is really weird,\n",
    "        ## Have to fix it a bit:\n",
    "        result['mid_latitude'] = float(monitor_row['latitude'].values[0])\n",
    "        # longitude:\n",
    "        result['mid_longitude'] = float(monitor_row['longitude'].values[0])\n",
    "        # loss:\n",
    "        result['mid_loss'] = float( 1 - monitor_row['loss'].values[0] )\n",
    "        # manufacturer api:\n",
    "        result['mid_manufacturerApi'] = monitor_row['manufacturerApi'].values[0] \n",
    "        # pvSize:\n",
    "        result['mid_pvsizewatt'] = monitor_row['pvSizeWatt'].values[0]\n",
    "        # tilt:\n",
    "        result['mid_tilt'] = float(monitor_row['tilt'].values[0])\n",
    "        # weatherstationid:\n",
    "        result['mid_weatherStationId'] = monitor_row['weatherStationId'].values[0]\n",
    "        # Azymuth:\n",
    "        result['mid_azimuth'] = float(monitor_row['azimuth'].values[0])\n",
    "\n",
    "        print(f'Fetching Metadata')\n",
    "    else:\n",
    "        print(f\"No data found for the Monitor ID {MID}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Helper functions to read metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metric(time_start, time_end, measure_name, MID):\n",
    "    \"\"\"\n",
    "    read raw data from the AWS database\n",
    "    :param time_start: time start, e.g., '2022-10-02'\n",
    "    :param time_end: time end, e.g., '2023-04-05'\n",
    "    :param measure_name: measurement metric, e.g.,'Gen.W'\n",
    "    :param MID: monitor id\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT time, measure_value::bigint\n",
    "                    FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                    WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                    AND MID = '\"\"\" + MID + \"\"\"'\n",
    "                    AND time BETWEEN '\"\"\" + time_start + \"\"\"'\n",
    "                    AND '\"\"\" + time_end + \"\"\"' \"\"\"\n",
    "\n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(timeid, measure_name, data_values, timezone_value):\n",
    "    \"\"\"\n",
    "    change the time zone\n",
    "    :param timeid: time read from the AWS\n",
    "    :param measure_name:\n",
    "    :param data_values: value read from the ASW\n",
    "    :param timezone_value: time zone\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    timeid = pd.to_datetime(timeid)\n",
    "    if timeid.tzinfo is None:\n",
    "        print('this is not tz-aware')\n",
    "        if timezone_value is not None:\n",
    "            timeid = timeid.tz_localize('UTC').tz_convert(timezone_value)\n",
    "        else:\n",
    "            print('no timezone in the table')\n",
    "            timeid = timeid.tz_localize('UTC').tz_convert('Australia/Sydney')\n",
    "    else:\n",
    "        print('this is tz-aware')\n",
    "    data = pd.DataFrame(data={'time':timeid, measure_name: data_values})\n",
    "    data.sort_values('time', inplace=True)\n",
    "    # data.set_index('time', inplace=True)\n",
    "    data[measure_name] = data[measure_name].astype(float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_tz(timeid):\n",
    "    # print('rawtimeid:', timeid)\n",
    "    tzinfo_str = timeid[0].tzinfo\n",
    "    hour_offset = tzinfo_str.utcoffset(datetime.datetime(2022,1,1))\n",
    "    hms = str(hour_offset).split(':')\n",
    "    time_modified = timeid + datetime.timedelta(hours=int(hms[0]), minutes=int(hms[1]), seconds=int(hms[2]))\n",
    "    time_utc = time_modified.dt.tz_convert('UTC')\n",
    "    # print('modified:', time_utc)\n",
    "    return time_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataframe(time_starttz, time_endtz):\n",
    "    print('-- Starting Dataframe Setup')\n",
    "\n",
    "    # Setting up a 5-min dataframe\n",
    "    time_index5min = pd.date_range(start=time_starttz, end=time_endtz, freq='5min').tz_convert('UTC')\n",
    "    df_5min = pd.DataFrame(index=np.arange(len(time_index5min)))\n",
    "    df_5min['time'] = time_index5min\n",
    "    return df_5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================#\n",
    "# = Reading P(AC) total from AWS TimeStream     #\n",
    "# = Metric is Gen.W                             #\n",
    "# ==============================================#\n",
    "\n",
    "def get_genW_dataframe(df_5min, time_start, time_end, measure_name, MID, timezone_value):\n",
    "    print(f'Reading from DyanmoDB and building Gen.W dataframe')\n",
    "    timeid, data_values = read_metric(time_start, time_end, measure_name, MID)\n",
    "    df_genW = build_dataframe(timeid, measure_name, data_values, timezone_value)\n",
    "\n",
    "    # =====================================================#\n",
    "    # = Adjusting the df_genW to have 5 minutes increments #\n",
    "    # =====================================================#\n",
    "\n",
    "    # Convert the 'time' column in df_5min to timezone\n",
    "    df_5min['time'] = df_5min['time'].dt.tz_convert(timezone_value)\n",
    "    df_genW = pd.merge_asof(df_5min, df_genW, on=\"time\")\n",
    "\n",
    "    # Getting the first valid index:\n",
    "    first_valid_index = df_genW['Gen.W'].first_valid_index()\n",
    "    df_genW = df_genW[first_valid_index:].reset_index(drop=True)\n",
    "\n",
    "    # making sure there's no negative generation\n",
    "    ## I'll be saving a copy of the original Gen.W to include an analysis of negative generation afterwards\n",
    "    df_genW['original_genW'] = df_genW['Gen.W']\n",
    "\n",
    "    # Now clipping\n",
    "    df_genW['Gen.W'] = df_genW['Gen.W'].clip(lower=0)\n",
    "    \n",
    "    return df_genW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Helper functions to get low-cloudiness days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metric_site(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe_site(timeid, measure_name, data_values):\n",
    "    # ============== Check if there is data available for the pv system =============\n",
    "    if len(timeid)!=0:\n",
    "        timeid = pd.to_datetime(timeid)\n",
    "        if timeid.tzinfo is None:\n",
    "            print('this is not tz-aware')\n",
    "            if timezone_value is not None:\n",
    "                timeid = timeid.tz_localize('UTC').tz_convert(timezone_value)\n",
    "                # timeid = timeid.tz_localize(timezone_list[i])\n",
    "            else:\n",
    "                print('no timezone in the table')\n",
    "                timeid = timeid.tz_localize('UTC').tz_convert('Australia/Sydney')\n",
    "                # timeid = timeid.tz_localize('Australia/Sydney')\n",
    "        else:\n",
    "            print('this is tz-aware')\n",
    "        \n",
    "        timesort = timeid.sort_values()\n",
    "        data = pd.DataFrame(data={'time':timeid, measure_name: data_values})\n",
    "        data.sort_values('time', inplace=True)\n",
    "        data.set_index('time', inplace=True)\n",
    "        data[measure_name] = data[measure_name].astype(float)\n",
    "    else:\n",
    "        data = pd.DataFrame(data_values, index=timeid, columns=[measure_name])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# = Merging clear skies and expected\n",
    "# ==================================\n",
    "\n",
    "def merge_clear_expe(df1, df2):\n",
    "    df_merged = df1.join(df2)\n",
    "    df_merged['expected_over_clear'] =  (df_merged['Irrad.kWh.m2.Daily'] / df_merged['EnergyYield.kWh.Daily'] * 100).round(0)\n",
    "    df_merged['date'] =  df_merged.index\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_process_cloudiness_data(time_start, time_end, site_id, threshold_low_cloudiness):\n",
    "        \n",
    "        # ====================================================#\n",
    "        # = Reading EnergyYield.kWh.Daily from AWS TimeStream #\n",
    "        # ====================================================#\n",
    "\n",
    "        print(f'Setting up low-cloudiness days')\n",
    "\n",
    "        print(f'-- Reading EnergyYield')\n",
    "        measure_name = 'EnergyYield.kWh.Daily'\n",
    "        timeid, data_values = read_metric_site(time_start, time_end, measure_name, site_id)\n",
    "        df_clear = build_dataframe_site(timeid, measure_name, data_values)\n",
    "\n",
    "        # =================================================#\n",
    "        # = Reading Irrad.kWh.m2.Daily from AWS TimeStream #\n",
    "        # =================================================#\n",
    "        \n",
    "        print(f'-- Reading Irrad')\n",
    "        measure_name = 'Irrad.kWh.m2.Daily'\n",
    "        timeid, data_values = read_metric_site(time_start, time_end, measure_name, site_id)\n",
    "        df_expected = build_dataframe_site(timeid, measure_name, data_values)\n",
    "\n",
    "        # Fixing it as a float:\n",
    "        df_expected['Irrad.kWh.m2.Daily'] = df_expected['Irrad.kWh.m2.Daily'].astype(float)\n",
    "\n",
    "        # ===================================================#\n",
    "        # = Reading Production.kWh.Daily from AWS TimeStream #\n",
    "        # ===================================================#\n",
    "\n",
    "        print(f'-- Reading Production Daily')\n",
    "        measure_name = 'Production.kWh.Daily'\n",
    "        timeid, data_values = read_metric_site(time_start, time_end, measure_name, site_id)\n",
    "        df_production = build_dataframe_site(timeid, measure_name, data_values)\n",
    "\n",
    "        # Fixing it as float\n",
    "        df_production['Production.kWh.Daily'] = df_production['Production.kWh.Daily'].astype(float)\n",
    "\n",
    "        # Merging clear and expected:\n",
    "        df_merged = df_clear.join(df_expected)\n",
    "\n",
    "        # Merging (clear and expected) and production\n",
    "        df_merged = df_merged.join(df_production)\n",
    "\n",
    "        # Getting the performance ratio:\n",
    "        df_merged['expected_over_clear'] =  (df_merged['Irrad.kWh.m2.Daily'] / df_merged['EnergyYield.kWh.Daily'] * 100).round(0)\n",
    "\n",
    "        # Getting this extra column flor plotting:\n",
    "        df_merged['date'] =  df_merged.index.date\n",
    "\n",
    "        # ====================================================================================#\n",
    "        # = Checking values above a certain threshold when comparing clear skies and expected #\n",
    "        # ====================================================================================#\n",
    "\n",
    "        print(f'-- Checking Cloudy Days')\n",
    "\n",
    "        # Make it low_cloudiness aware:\n",
    "        df_merged.loc[df_merged['expected_over_clear'] >= threshold_low_cloudiness, 'is_low_cloudiness_day'] = True \n",
    "        df_merged.loc[df_merged['expected_over_clear'] < threshold_low_cloudiness, 'is_low_cloudiness_day'] = False\n",
    "\n",
    "        return df_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data from PVLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://pvlib-python.readthedocs.io/en/stable/user_guide/clearsky.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irradiance(loc, times, tilt, surface_azimuth):\n",
    "    # Generate clearsky data using the Ineichen model, which is the default\n",
    "    # The get_clearsky method returns a dataframe with values for GHI, DNI,and DHI\n",
    "    clearsky = loc.get_clearsky(times)\n",
    "    # Get solar azimuth and zenith to pass to the transposition function\n",
    "    solar_position = loc.get_solarposition(times=times)\n",
    "    # Use the get_total_irradiance function to transpose the GHI to POA\n",
    "    POA_irradiance = irradiance.get_total_irradiance(\n",
    "        surface_tilt=tilt,\n",
    "        surface_azimuth=surface_azimuth,\n",
    "        dni=clearsky['dni'],\n",
    "        ghi=clearsky['ghi'],\n",
    "        dhi=clearsky['dhi'],\n",
    "        solar_zenith=solar_position['apparent_zenith'],\n",
    "        solar_azimuth=solar_position['azimuth'])\n",
    "    # Return DataFrame with only GHI and POA\n",
    "    return pd.DataFrame({'GHI': clearsky['ghi'],\n",
    "                         'POA': POA_irradiance['poa_global']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sun_and_irradiance(df_genW, timezone_value, monitor_metadata, time_start, time_end):\n",
    "        \n",
    "        # Every day only has 1 sunrise and 1 sunset, so we can go with uique days:\n",
    "        unique_dates = df_genW['time'].dt.date.unique()\n",
    "\n",
    "        # initialising:\n",
    "        sun_info = {}\n",
    "        # iterating over my array of uniquedates:\n",
    "        for date in unique_dates:\n",
    "            # pvlib expects a localised value, not an iterable list, so I'm wrapping my date timestamp:\n",
    "            localized_date = pd.DatetimeIndex([pd.Timestamp(date).tz_localize(timezone_value)]) \n",
    "            # getting the results, which is a 1 row dataframe with 3 columns:\n",
    "            sun_results = pvlib.solarposition.sun_rise_set_transit_spa(localized_date, monitor_metadata['mid_latitude'], monitor_metadata['mid_longitude'])\n",
    "            sun_info[date] = sun_results.values[0] \n",
    "\n",
    "        # Convert dictionary to a DataFrame:\n",
    "        sun_info_df = pd.DataFrame.from_dict(sun_info, orient='index', columns=['sunrise', 'sunset', 'transit'])\n",
    "\n",
    "        # creating a date column for the merge:\n",
    "        df_genW['date'] = df_genW['time'].dt.date\n",
    "\n",
    "        # Reset index in sun_info_df and rename the index column as 'date'\n",
    "        sun_info_df = sun_info_df.reset_index().rename(columns={'index':'date'})\n",
    "\n",
    "        # Convert the 'date' column to datetime in both dataframes\n",
    "        df_genW['date'] = pd.to_datetime(df_genW['date'])\n",
    "        sun_info_df['date'] = pd.to_datetime(sun_info_df['date'])\n",
    "\n",
    "        # Merge df_genW with sun_info_df\n",
    "        df_genW = pd.merge(df_genW, sun_info_df, on='date', how='left')\n",
    "\n",
    "        # Convert 'time' column to datetime, to be sure:\n",
    "        df_genW['time'] = pd.to_datetime(df_genW['time'])\n",
    "\n",
    "        # Create a copy of 'time' column before setting it as index, I'll need this for the labelling\n",
    "        df_genW['time_copy'] = df_genW['time']\n",
    "\n",
    "        # Set 'time' column as index\n",
    "        df_genW.set_index('time', inplace=True)\n",
    "\n",
    "        # Running pvlib to get GHI:\n",
    "        location = pvlib.location.Location(monitor_metadata['mid_latitude'], monitor_metadata['mid_longitude'], tz=timezone_value)\n",
    "        df_genW['clear_sky_ghi'] = location.get_clearsky(df_genW.index, model='ineichen')['ghi']\n",
    "\n",
    "        # Renaming time_copy to time again:\n",
    "        df_genW.rename(columns={'time_copy': 'time'}, inplace=True)\n",
    "\n",
    "        # ================================================================#\n",
    "        # = Getting POA and GHI using PVLib, and setting up the dataframe #\n",
    "        # ================================================================#\n",
    "\n",
    "        # Setting up a dataframe with local info:\n",
    "        time_index5min_local = pd.date_range(start=pd.to_datetime(time_start).tz_localize(timezone_value), end=pd.to_datetime(time_end).tz_localize(timezone_value), freq='5min')\n",
    "\n",
    "        # Getting the location\n",
    "        loc = pvlib.location.Location(monitor_metadata['mid_latitude'], monitor_metadata['mid_longitude'], tz=timezone_value)\n",
    "        pvlib_irr_pre = get_irradiance(loc, time_index5min_local, monitor_metadata['mid_tilt'], monitor_metadata['mid_azimuth'])\n",
    "\n",
    "        # Correcting for size and loss\n",
    "        pvlib_irr_pre['POA'] = pvlib_irr_pre['POA'] * monitor_metadata['mid_pvsizewatt'] * monitor_metadata['mid_loss'] / 1000\n",
    "\n",
    "        # Merging the dataframes\n",
    "        merged_df = pd.merge(df_genW, pvlib_irr_pre, left_index=True, right_index=True)\n",
    "\n",
    "        # Assign the column\n",
    "        merged_df['theoretical_clear-sky_generation.W'] = merged_df['POA']\n",
    "\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transformations for recurring fault analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_and_threshold(df_genW):\n",
    "    df_genW_resample = df_genW.copy()  # create a copy of the original dataframe\n",
    "\n",
    "    df_genW_resample['time'] = pd.to_datetime(df_genW_resample['time']) # convert time to datetime if it's not\n",
    "    df_genW_resample.set_index('time', inplace=True) # set time as index\n",
    "\n",
    "    # Create new df with resampling\n",
    "    df_genW_resample = df_genW_resample.resample('H').mean()\n",
    "\n",
    "    return df_genW_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that cadence_of_observation is in minutes.\n",
    "\n",
    "def shift_timeseries(df,cadence_of_observation = 5):\n",
    "    # Finding the first non-zero generation timestamp for each DataFrame\n",
    "    ## Measured\n",
    "    start_genW = df[df['Gen.W'] != 0]['time'].iloc[0]\n",
    "    ## Clear Skies\n",
    "    start_theoretical = df[df['theoretical_clear-sky_generation.W'] != 0]['time'].iloc[0]\n",
    "\n",
    "    # Compute time difference in hours\n",
    "    time_diff = (start_theoretical - start_genW).total_seconds() / 3600\n",
    "\n",
    "    # Converting timedelta into minutes\n",
    "    shift_minutes = timedelta(hours=time_diff).seconds // 60\n",
    "\n",
    "    print(f'shifting the timeseries by {shift_minutes} minutes')\n",
    "\n",
    "    shift_periods = int(shift_minutes / cadence_of_observation)\n",
    "\n",
    "    # Shift 'Gen.W' column in df_genW_copy\n",
    "    df['Gen.W'] = df['Gen.W'].shift(shift_periods)\n",
    "\n",
    "    # Fill NaN values (if any) with 0 after shifting\n",
    "    df['Gen.W'].fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize columns based on the maximum value of theoretical_clear-sky_generation.W for each day\n",
    "def normalize_daywise(group):\n",
    "\n",
    "    # CUrrently hardcoded Gen.W -> Make sure to change this to a if stament to encompass the cases for when\n",
    "    # On a day Gen.W has a higher max and on a day Theoritical has a higher max\n",
    "\n",
    "    max_val = max(group['Gen.W'].max(), group['theoretical_clear-sky_generation.W'].max())\n",
    "\n",
    "    # Avoiding division by zero and infinite results tehreafter:\n",
    "    max_val = max_val if max_val != 0 else 1\n",
    "    \n",
    "    group['theoretical_clear-sky_generation.W_normalized'] = group['theoretical_clear-sky_generation.W'] / max_val\n",
    "    \n",
    "    group['Gen.W_normalized'] = group['Gen.W'] / max_val\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_theoretical(df):\n",
    "\n",
    "    # I'm initialising an empty dict so I can keep track of the stretch factors:\n",
    "    stretch_factors = {}\n",
    "\n",
    "    # Threshold for theoretical value below which we won't compute the ratio\n",
    "    ## The threshold helps in avoiding unrealistically high stretch factors caused by near-zero values in the denominator.\n",
    "    \n",
    "    ## Currently using 0.7, as 70% of the main normalised value (1)\n",
    "    THRESHOLD = 0.7\n",
    "\n",
    "    # Function to compute and adjust the theoretical curve for each day's group\n",
    "    def compute_stretch(group):\n",
    "\n",
    "        # Check if the input is a DataFrame and if not, return as is\n",
    "        ## I've been getting some weird AttributeErrors here\n",
    "        if not isinstance(group, pd.DataFrame):\n",
    "            return group\n",
    "\n",
    "        # getting the date from each row, so I can keep track of stretch factors\n",
    "        current_date = group.index[0].date()\n",
    "\n",
    "        # Compute ratio where theoretical value is above the threshold\n",
    "        ## I want to make sure that I'm not dividing by tiny values and getting crazy spikes\n",
    "        valid_idxs = group['theoretical_clear-sky_generation.W_normalized'] > THRESHOLD\n",
    "        ratios = (group['Gen.W_normalized'] / group['theoretical_clear-sky_generation.W_normalized'])[valid_idxs]\n",
    "        \n",
    "        # Use maximum ratio as stretch factor\n",
    "        stretch_factor = ratios.max()\n",
    "        \n",
    "        # If the stretch factor is NaN or zero (no valid ratios), set it to 1 to avoid NaN results\n",
    "        # This will keep the value unchanged\n",
    "        stretch_factor = 1 if pd.isna(stretch_factor) or stretch_factor == 0 else stretch_factor\n",
    "\n",
    "        # Storing each stretch factor so I can keep track of it:\n",
    "        stretch_factors[current_date] = stretch_factor\n",
    "\n",
    "        # Multiply each value in the theoretical column by the stretched factor\n",
    "        group['stretched_theoretical'] = group['theoretical_clear-sky_generation.W_normalized'] * stretch_factor\n",
    "        return group\n",
    "\n",
    "    # I'm grouping the datasframe by DAY and applying that function to it\n",
    "    df_stretched = df.groupby(df.index.map(lambda x: x.date())).apply(compute_stretch)\n",
    "\n",
    "    # Convert the stretch_factors dictionary to a DataFrame for better vis\n",
    "    stretch_factors_df = pd.DataFrame(list(stretch_factors.items()), columns=['date', 'stretch_factor'])\n",
    "    # Convert 'date' column to datetime and set as index\n",
    "    stretch_factors_df['date'] = pd.to_datetime(stretch_factors_df['date'])\n",
    "    stretch_factors_df.set_index('date', inplace=True)\n",
    "\n",
    "    return df_stretched, stretch_factors_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deltas(df_genW_resample, df_stretched):\n",
    "        \n",
    "    # =================#\n",
    "    # = Getting Deltas #\n",
    "    # =================#\n",
    "\n",
    "    # Getting deltas\n",
    "    df_genW_resample['time'] = df_genW_resample.index\n",
    "    df_stretched['delta-clear-gen'] = df_stretched['Gen.W_normalized'] - df_stretched['stretched_theoretical']\n",
    "\n",
    "    # simplifying\n",
    "    df_MA = df_stretched.copy(deep=True)\n",
    "    df_MA = df_MA.loc[:,['time','Gen.W','theoretical_clear-sky_generation.W','delta-clear-gen','Gen.W_normalized','stretched_theoretical']]\n",
    "    df_MA = df_MA.resample('H').sum()\n",
    "    df_MA['timestamp'] = df_MA.index\n",
    "    df_MA = df_MA.rename(columns={\"delta-clear-gen\": \"value\"})\n",
    "    \n",
    "    # inverting\n",
    "    df_MA_inverted = df_MA.copy(deep=True)\n",
    "    df_MA_inverted['value'] = df_MA['value'] * (-1)\n",
    "\n",
    "    return df_MA_inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_on_low_cloudiness(df_MA_inverted, df_site):\n",
    "    # Convert 'timestamp' column in df_test to datetime if it's not\n",
    "    df_MA_inverted['timestamp'] = pd.to_datetime(df_MA_inverted['timestamp'])\n",
    "\n",
    "    # Create a new column in df_test with just the date (no time info)\n",
    "    df_MA_inverted['date_only'] = df_MA_inverted['timestamp'].dt.date\n",
    "\n",
    "    # Convert index 'time' to datetime and create a new column 'time' in df_site if it's not\n",
    "    df_site['time'] = pd.to_datetime(df_site.index)\n",
    "\n",
    "    # Create a new column in df_site with just the date (no time info)\n",
    "    df_site['date_only'] = df_site['time'].dt.date\n",
    "\n",
    "    # Merge df_site and df_test on the date_only column\n",
    "    df_with_cloud = df_MA_inverted.merge(df_site[['date_only', 'is_low_cloudiness_day']], on='date_only', how='left')\n",
    "\n",
    "    # If you want to drop the 'date_only' column after the merge:\n",
    "    df_with_cloud = df_with_cloud.drop(columns=['date_only'])\n",
    "\n",
    "    # Create a copy of 'timestamp' column\n",
    "    df_with_cloud['timestamp_copy'] = df_with_cloud['timestamp']\n",
    "\n",
    "    # Set 'timestamp' column as index\n",
    "    df_with_cloud.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # Rename the 'timestamp_copy' back to 'timestamp'\n",
    "    df_with_cloud.rename(columns={'timestamp_copy': 'timestamp'}, inplace=True)\n",
    "\n",
    "    # Deep copy\n",
    "    df_MA_inverted = df_with_cloud.copy(deep=True)\n",
    "\n",
    "    return df_MA_inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_to_threshold(df, threshold_percentage=0.2):\n",
    "    # Make sure index is a DateTimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Get all unique dates in the df\n",
    "    unique_dates = df.index.normalize().unique()\n",
    "\n",
    "    # Iterate over each unique day in the df\n",
    "    for date in unique_dates:\n",
    "        # Convert to string for indexing\n",
    "        str_date = date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Get the max 'value' for the day\n",
    "        max_value = df.loc[str_date, 'stretched_theoretical'].max()\n",
    "\n",
    "        # Multiply the max 'value' by the threshold\n",
    "        threshold = max_value * threshold_percentage\n",
    "\n",
    "        # Find 'Gen.W' values that are below the threshold and replace them with 0\n",
    "        df.loc[(df.index.normalize() == date) & (df['value'] < threshold), 'value'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_recurrent_underperformance(df, threshold_of_recurrence=3, value_threshold_percentage=20):\n",
    "    \n",
    "    if df.index.name != 'timestamp':\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    df['hour'] = df.index.hour\n",
    "    df['underperformance'] = df['value'] > 0  # Assuming 'value' > 0 means underperformance\n",
    "    df['Recurring Underperformance'] = False\n",
    "    df['count_for_hour_of_day'] = 0\n",
    "    \n",
    "    # Filter only rows where is_low_cloudiness_day is True\n",
    "    low_cloudiness_df = df[df['is_low_cloudiness_day'].fillna(False)]\n",
    "    \n",
    "    for hour in range(24):  # Looping through all hours of the day\n",
    "        hourly_df = low_cloudiness_df[low_cloudiness_df['hour'] == hour].copy()\n",
    "        \n",
    "        count = 0\n",
    "        potential_indices = []  # To keep track of the potential recurrent underperformances\n",
    "        \n",
    "        for idx, row in hourly_df.iterrows():\n",
    "            \n",
    "            if row['underperformance'] and row['value'] != 0:\n",
    "                \n",
    "                # For the first encounter with a fault\n",
    "                if not potential_indices:  # List is empty, initialize\n",
    "                    count = 1\n",
    "                    potential_indices = [idx]\n",
    "                    df.loc[idx, 'count_for_hour_of_day'] = count\n",
    "                    reference_value = row['value']\n",
    "                    \n",
    "                else:\n",
    "                    # Compute the percentage difference\n",
    "                    reference_value = hourly_df.loc[potential_indices[0], 'value']\n",
    "                    percentage_diff = abs((row['value'] - reference_value) / reference_value) * 100\n",
    "                    \n",
    "                    # Check if the value is within the threshold\n",
    "                    if percentage_diff <= value_threshold_percentage:\n",
    "                        count += 1\n",
    "                        df.loc[idx, 'count_for_hour_of_day'] = count\n",
    "                        potential_indices.append(idx)  # Store this index\n",
    "                    else:\n",
    "                        # Reset count and start tracking from the current index\n",
    "                        count = 1  \n",
    "                        df.loc[idx, 'count_for_hour_of_day'] = count\n",
    "                        potential_indices = [idx]  # Reset potential_indices list\n",
    "\n",
    "                # Check if count reaches the threshold\n",
    "                if count >= threshold_of_recurrence:\n",
    "                    # Set 'is_recurrent_underperformance' to True for all potential indices\n",
    "                    df.loc[potential_indices, 'Recurring Underperformance'] = True\n",
    "                \n",
    "            else:\n",
    "                # Reset count and clear list\n",
    "                count = 0  \n",
    "                potential_indices = []  \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Functions to label Level 1 faults based on raw signal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Generation Tripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_solar_tripping(df, trip_threshold=3):\n",
    "    # Ensure index is in the correct format\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['date_from_time'] = df['time'].dt.date\n",
    "    \n",
    "    # Create a boolean mask where generation is zero\n",
    "    df['is_zero_gen'] = df['Gen.W'] == 0\n",
    "\n",
    "    # Restrict to periods between sunrise and sunset\n",
    "    df['is_zero_gen'] = df['is_zero_gen'] & \\\n",
    "                                        (((df.index.hour * 60) + df.index.minute)>= df['sun_thre_start']) & (((df.index.hour * 60) + df.index.minute) <= df['sun_thre_end'])\n",
    "\n",
    "    # Identifying instances where generation goes to zero and then back up\n",
    "\n",
    "        ## First identify where is_zero_gen changes from False to True or True to False\n",
    "    df['diff_is_zero_gen'] = df['is_zero_gen'].astype(int).diff()\n",
    "\n",
    "        ## Then isolate instances where the generation is greater than 0\n",
    "    df['positive_gen'] = df['Gen.W'] > 0\n",
    "\n",
    "        ## Finally, combining both conditions\n",
    "    df['is_tripping'] = (df['diff_is_zero_gen'] != 0) & df['positive_gen']\n",
    "\n",
    "    # Get daily counts of tripping\n",
    "    daily_tripping_counts = df.resample('D')['is_tripping'].sum()\n",
    "\n",
    "    # Identify the dates where tripping counts cross the threshold\n",
    "    threshold_dates = daily_tripping_counts[daily_tripping_counts >= trip_threshold].index.date\n",
    "\n",
    "    # Convert df.index.date to a pandas Series\n",
    "    df_dates = pd.Series(df.index.date)\n",
    "    \n",
    "    # Update 'fault_tripping' column based on threshold_dates\n",
    "    df['fault_tripping'] = df['date_from_time'].isin(threshold_dates)\n",
    "\n",
    "    # I'm filling NaN values with False\n",
    "    df['fault_tripping'] = df['fault_tripping'].fillna(False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Non-zero generation tripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nonZero_tripping(df, threshold_nonZero_tripping=0.8):\n",
    "    # Initiatlising the column as False\n",
    "    df['nonZero_tripping'] = 0  \n",
    "    df['fault_nonZero_tripping'] = False  \n",
    "\n",
    "    # looping through and getting individual drops:\n",
    "    for i in range(1, len(df)):\n",
    "        # Get a ratio from measurements right after another\n",
    "        ratio = df['Gen.W'].iloc[i] / df['Gen.W'].iloc[i-1]\n",
    "\n",
    "        # If a drop is too significant, consider it a non-zero tripping:\n",
    "        if ratio < threshold_nonZero_tripping:\n",
    "            df['nonZero_tripping'].iloc[i] = 1\n",
    "\n",
    "    # Summing up all drops on a given day:\n",
    "    df['nonZero_tripping_sum'] = df['nonZero_tripping'].resample('D').transform('sum')\n",
    "\n",
    "    # Marking a day that this happens at least thrice:\n",
    "    df.loc[df['nonZero_tripping_sum'] >= 3, 'fault_nonZero_tripping'] = True\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Generation Clipping - Normalised with PVSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_solar_clipping(df, mid_pvsizewatt, threshold=0.001, min_duration='60min'):\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Calculate the rate of change of power\n",
    "    # Currently using diff, should use derivative?\n",
    "    \n",
    "    df['power_diff_normalised'] = df['Gen.W'].diff()/mid_pvsizewatt\n",
    "\n",
    "    # We only want to check clipping in between the thresholds, therefore we'll need to define the hour:\n",
    "    # df['hour'] = df.index.hour\n",
    "\n",
    "    # Identifying periods where:\n",
    "    ##  the rate of change is near zero (potential clipping, considered by threshold), and \n",
    "    ## there more than 100 generation (sun_threshold), and\n",
    "    ## is between sun_thre_start and sun_thre_end\n",
    "    df['is_clipping'] = (np.abs(df['power_diff_normalised']) < threshold) & \\\n",
    "                        (df['Gen.W'] > 100) & \\\n",
    "                        (((df.index.hour * 60) + df.index.minute)>= df['sun_thre_start']) & (((df.index.hour * 60) + df.index.minute) <= df['sun_thre_end'])\n",
    "    \n",
    "    # Identify 'clipping periods', which are periods of clipping that last at least min_duration\n",
    "    ## First we get EVERY occurrence of clipping, for every 5 min\n",
    "    df['clipping_period'] = df['is_clipping'].diff().ne(0).cumsum()\n",
    "\n",
    "    # Then, we'll calculate the duration of each 'clipping period'\n",
    "    df['clipping_duration'] = df.groupby('clipping_period')['is_clipping'].transform('sum') * (df.index.to_series().diff().dt.total_seconds() / 60)\n",
    "\n",
    "    # If duration is longer than our min_duration, we label that as clipping\n",
    "    df['fault_clipping'] = df['is_clipping'] & (df['clipping_duration'] >= pd.Timedelta(min_duration).total_seconds() / 60)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Zero Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_zero_generation(df, min_duration='60min'):  \n",
    "    # Creating a boolean mask where generation is zero\n",
    "    # tbd -> optimise this with tripping\n",
    "    df['is_zero_gen'] = df['Gen.W'] == 0\n",
    "\n",
    "    # Restrict to periods between sunrise and sunset\n",
    "    df['is_zero_gen'] = df['is_zero_gen'] & (((df.index.hour * 60) + df.index.minute)>= df['sun_thre_start']) & (((df.index.hour * 60) + df.index.minute) <= df['sun_thre_end'])\n",
    "\n",
    "    '''   \n",
    "        # Trying with zero_gen and ghi not zero:\n",
    "        df['is_zero_gen'] = df['is_zero_gen'] & df['clear_sky_ghi'] > 0\n",
    "    '''\n",
    "\n",
    "    # Labeling periods of zero generation\n",
    "    df['zero_gen_period'] = df['is_zero_gen'].diff().ne(0).cumsum()\n",
    "\n",
    "    # Calculate the duration of each 'zero_gen_period'\n",
    "    df['zero_gen_duration'] = df.groupby('zero_gen_period')['is_zero_gen'].transform('sum') * (df.index.to_series().diff().dt.total_seconds() / 60)\n",
    "\n",
    "    # If duration is longer than our min_duration, we label that as zero_gen_period\n",
    "    df['fault_zero_gen'] = df['is_zero_gen'] & (df['zero_gen_duration'] >= pd.Timedelta(min_duration).total_seconds() / 60)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Recurring underperformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Night-time Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nightTime_gen(df):\n",
    "    # Create a boolean column for generation during night time\n",
    "    df['temp_fault_nightTime_gen'] = (df['Gen.W'] > 0) & \\\n",
    "                                     ~((((df.index.hour * 60) + df.index.minute) >= df['sun_thre_start']) & \\\n",
    "                                       (((df.index.hour * 60) + df.index.minute) <= df['sun_thre_end']))\n",
    "\n",
    "    # Create a new column to indicate if the nighttime generation occurred continuously for 1 hour (12 intervals)\n",
    "    df['fault_nightTime_gen_1hr'] = df['temp_fault_nightTime_gen'].rolling(window=12, min_periods=12).sum() == 12\n",
    "\n",
    "    # Drop the temporary column\n",
    "    df.drop(columns=['temp_fault_nightTime_gen'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Negative Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, need to use df_toCheck_negativeGen\n",
    "\n",
    "def detect_negative_gen(df, mid_pvsizewatt):\n",
    "\n",
    "    # Calculating the threshold (1% of mid_pvsizewatt)\n",
    "    threshold_neg = 0.01 * mid_pvsizewatt\n",
    "\n",
    "    # Creating a boolean to catch negative values\n",
    "    df['fault_negative_gen'] = (df['original_genW'] < 0) & (abs(df['original_genW']) >= threshold_neg)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8. Excessive Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_excessive_gen(df, mid_pvsizewatt):\n",
    "\n",
    "    # 100% on top of system size\n",
    "    threshold_excessive = 2 * mid_pvsizewatt\n",
    "\n",
    "    df['fault_excessive_gen'] = (df['Gen.W'] >= threshold_excessive)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9. No Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_no_data(df):\n",
    "    df['No Data'] = False\n",
    "    if df['Gen.W'].isna().all():\n",
    "        df['No Data'] = True\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Label all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dataframe(df):\n",
    "    # Extract hour from sunrise and sunset times\n",
    "\n",
    "    # Extract time from sunrise and sunset times\n",
    "    ## I was getting false positives since the sunset could be at 17:01, and by extracting the hour from the timestamp, I would get 17, and in that case I would have 12 possible slots to detect fault.\n",
    "    ## I'm doing a 1 hour buffer for the start, and flooring the sunset\n",
    "    df['index_minute_vale'] = ((df_genW.index.hour * 60) + df_genW.index.minute)\n",
    "    df['sun_thre_start'] = (((df['sunrise'].dt.hour + 1) % 24) * 60) + df['sunrise'].dt.minute\n",
    "    df['sun_thre_end'] = (df['sunset'].dt.hour * 60) + df['sunrise'].dt.minute\n",
    "    \n",
    "    df = detect_solar_tripping(df)\n",
    "    df = detect_nonZero_tripping(df)\n",
    "    df = detect_solar_clipping(df, monitor_metadata['mid_pvsizewatt'])\n",
    "    df = detect_zero_generation(df)\n",
    "    df = detect_negative_gen(df, monitor_metadata['mid_pvsizewatt'])\n",
    "    df = detect_nightTime_gen(df)\n",
    "    df = detect_excessive_gen(df, monitor_metadata['mid_pvsizewatt'])\n",
    "    df = detect_no_data(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faults(row):\n",
    "    faults = []\n",
    "    if row['fault_tripping']:\n",
    "        faults.append('fault_tripping')\n",
    "    if row['fault_nonZero_tripping']:\n",
    "        faults.append('fault_nonZero_tripping')\n",
    "    if row['fault_clipping']:\n",
    "        faults.append('fault_clipping')\n",
    "    if row['fault_zero_gen']:\n",
    "        faults.append('fault_zero_gen')\n",
    "    if row['fault_recurrent_underperformance']:\n",
    "        faults.append('fault_recurrent_underperformance')\n",
    "    if row['fault_negative_gen']:\n",
    "        faults.append('Negative Generation')\n",
    "    if row['fault_nightTime_gen_1hr']:\n",
    "        faults.append('Night-Time Generation')\n",
    "    if row['fault_excessive_gen']:\n",
    "        faults.append('Excessive Generation')\n",
    "    if row['No Data']:\n",
    "        faults.append('No Data')\n",
    "    return faults"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Saving individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# To save it:\n",
    "df_labelled.to_csv(f'./2B_monitor_results/individual_monitors/{MID}.csv')\n",
    "\n",
    "print(f'Fault Detection saved for {MID}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_per_day = df_labelled.copy()\n",
    "\n",
    "df_per_day.index = df_per_day.index.date\n",
    "\n",
    "def agg_faults_per_day(faults_list):\n",
    "    unique_faults_on_that_day = list(set([fault for sublist in faults_list for fault in sublist]))\n",
    "    return unique_faults_on_that_day\n",
    "\n",
    "result_per_day = df_per_day.groupby(df_per_day.index).agg({'Gen.W': 'sum', 'faults': agg_faults_per_day})\n",
    "\n",
    "result_per_day.to_csv(f'./2B_monitor_results/per_day/{MID}_per_day.csv')'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Aggregate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(monitors_list)):\n",
    "    try:\n",
    "\n",
    "        # =======================================================#\n",
    "        #  1. Initialisation + Site and Monitor data collection  #\n",
    "        # =======================================================#\n",
    "\n",
    "        site_id_full, site_id, MID_full, MID = get_mid_info(monitors_list, i)\n",
    "\n",
    "        print(f'Beginning Analysis for Monitor: {MID} under {site_id_full}')\n",
    "\n",
    "        timezone_value, time_starttz, time_endtz = get_timezones(site_id_full, sites_list, time_start, time_end)\n",
    "\n",
    "        monitor_metadata = fetch_monitor_metadata(monitors_list)\n",
    "\n",
    "        # =========================================#\n",
    "        #  2. Building a Gen.W dataframe from AWS  #\n",
    "        # =========================================#\n",
    "\n",
    "        df_5min = setup_dataframe(time_starttz, time_endtz)\n",
    "        df_genW = get_genW_dataframe(df_5min, time_start, time_end, 'Gen.W', MID, timezone_value)\n",
    "\n",
    "        # ===============================================================#\n",
    "        # = Resampling a 1-hour dataframe for recurrent underperformance #\n",
    "        # ===============================================================#\n",
    "\n",
    "        # Creating a copy of the 'time' column\n",
    "        df_genW['time_copy'] = df_genW['time']\n",
    "\n",
    "        # Setting the 'time' column as the index\n",
    "        df_genW = df_genW.set_index('time')\n",
    "\n",
    "        # Resampling the dataframe hourly\n",
    "        df_genW_resampled = df_genW.resample('H').mean()\n",
    "\n",
    "        # Reseting the index\n",
    "        df_genW_resampled.reset_index(level=0, inplace=True)\n",
    "        df_genW.reset_index(level=0, inplace=True)\n",
    "\n",
    "        # ================================#\n",
    "        # = Information on low-cloudiness #\n",
    "        # ================================#\n",
    "\n",
    "        df_site = fetch_and_process_cloudiness_data(time_start, time_end, site_id, threshold_low_cloudiness)\n",
    "\n",
    "        # =================================================#\n",
    "        # = Using PVLib to get sunrise, sunset and transit #\n",
    "        # =================================================#\n",
    "\n",
    "        df_genW = compute_sun_and_irradiance(df_genW, timezone_value, monitor_metadata, time_start, time_end)\n",
    "\n",
    "        # ====================================================================#\n",
    "        # = Adding an hour resample to analyse the recurrent underperformance #\n",
    "        # ====================================================================#\n",
    "\n",
    "        df_genW_resample = df_genW.copy()  # create a copy of the original dataframe\n",
    "\n",
    "        df_genW_resample['time'] = pd.to_datetime(df_genW_resample['time']) # convert time to datetime if it's not\n",
    "        df_genW_resample.set_index('time', inplace=True) # set time as index\n",
    "\n",
    "        # Create new df with resampling\n",
    "        df_genW_resample = df_genW_resample.resample('H').mean()\n",
    "\n",
    "        # ================================#\n",
    "        # = Shifting resampled timeseries #\n",
    "        # ================================#\n",
    "\n",
    "        # Note that before shifting I want to apply a threshold on the aggregate hourly dataframe before shifting, to avoid situations in which there's more rows of measured than theoretical\n",
    "        threshold_minimum_generation = df_genW_resample['Gen.W'].max() * 0.01\n",
    "        threshold_minimum_theoretical = df_genW_resample['theoretical_clear-sky_generation.W'].max() * 0.01\n",
    "\n",
    "        df_genW_resample.loc[df_genW_resample['Gen.W'] <  threshold_minimum_generation, 'Gen.W'] = 0\n",
    "        df_genW_resample.loc[df_genW_resample['theoretical_clear-sky_generation.W'] <  threshold_minimum_theoretical, 'theoretical_clear-sky_generation.W'] = 0\n",
    "\n",
    "        threshold_minimum_theoretical = df_genW_resample['theoretical_clear-sky_generation.W'].max() * 0.01\n",
    "        df_genW_resample.loc[df_genW_resample['theoretical_clear-sky_generation.W'] <  threshold_minimum_theoretical, 'theoretical_clear-sky_generation.W'] = 0\n",
    "\n",
    "\n",
    "        df_genW_resample['time'] = df_genW_resample.index\n",
    "        df_genW_resample = shift_timeseries(df_genW_resample, cadence_of_observation=5)\n",
    "\n",
    "\n",
    "        # ===================================#\n",
    "        # = Normalising resampled timeseries #\n",
    "        # ===================================#\n",
    "        \n",
    "        # Group by day and apply normalization\n",
    "        df_normalized = df_genW_resample.groupby(df_genW_resample.index.date).apply(normalize_daywise)\n",
    "\n",
    "        # ==================================#\n",
    "        # = Stretching resampled timeseries #\n",
    "        # ==================================#\n",
    "\n",
    "        # Stretching\n",
    "        df_stretched, stretch_factors_df = stretch_theoretical(df_normalized)\n",
    "\n",
    "        # The values of 'theoretical_clear-sky_generation.W' should never be bigger than 'Gen.W'\n",
    "        # For now, I'm using Gen.W as the cap\n",
    "        df_stretched['Gen.W_normalized'] = np.where(df_stretched['Gen.W_normalized'] > df_stretched['stretched_theoretical'], df_stretched['stretched_theoretical'], df_stretched['Gen.W_normalized'])\n",
    "    \n",
    "        # =================#\n",
    "        # = Getting Deltas #\n",
    "        # =================#\n",
    "\n",
    "\n",
    "        df_MA_inverted = get_deltas(df_genW_resample, df_stretched)\n",
    "\n",
    "        # ==============================#\n",
    "        # = Getting low-cloudiness info #\n",
    "        # ==============================#\n",
    "\n",
    "        df_MA_inverted = merge_on_low_cloudiness(df_MA_inverted, df_site)\n",
    "\n",
    "        # ===============================#\n",
    "        # = Limiting deltas to threshold #\n",
    "        # ===============================#\n",
    "\n",
    "        # Limiting to threshold\n",
    "        df_MA_inverted = limit_to_threshold(df_MA_inverted)\n",
    "\n",
    "\n",
    "        # ====================================#\n",
    "        # = Maring recurrent underperformance #\n",
    "        # ====================================#\n",
    "\n",
    "        mark_recurrent_underperformance(df_MA_inverted)\n",
    "\n",
    "        # ================#\n",
    "        # = Labelling all #\n",
    "        # ================#\n",
    "\n",
    "        # I've already worked with df_MA_inverted, I'll just use it to label the original df_genW\n",
    "\n",
    "        df_recurrent_und = df_MA_inverted.copy(deep=True)\n",
    "        df_recurrent_und = df_recurrent_und[['timestamp','Recurring Underperformance']].rename(columns={'timestamp':'time'})\n",
    "\n",
    "        # df_MA_inverted is a hourly dataframe, whereas df_genW has 5-minutes increments\n",
    "        # df_MA_inverted['Recurring Underperformance'] is TRUE on an hourly basis:\n",
    "        ## If it happens to be TRUE on a full hour, I'll propagate it to it's corresponding 5-minutes increments:\n",
    "        # Using forward fill to achieve this:\n",
    "\n",
    "        df_recurrent_und = df_recurrent_und.resample('5T').ffill()\n",
    "\n",
    "        df_labelled = label_dataframe(df_genW)\n",
    "        \n",
    "        # ====================================#\n",
    "        # = Labelling all #\n",
    "        # ====================================#\n",
    "\n",
    "        # I'll have to resample the df_MA_inverted to 5 minutes intervals\n",
    "        # The only thing I really care about here is the 'Recurring Underperformance' column, so I'll use forward fill:\n",
    "        # https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.ffill.html\n",
    "\n",
    "\n",
    "        df_labelled['time'] = pd.to_datetime(df_labelled['time'])\n",
    "        df_MA_inverted.index = pd.to_datetime(df_MA_inverted.index)\n",
    "\n",
    "        df_recurrent_und = df_MA_inverted.copy(deep=True)\n",
    "        df_recurrent_und = df_recurrent_und[['timestamp','Recurring Underperformance', 'is_low_cloudiness_day']].rename(columns={'timestamp':'time'})\n",
    "\n",
    "        # Resample the df_MA_inverted to 5-minute intervals using forward filling\n",
    "        df_recurrent_und = df_recurrent_und.resample('5T').ffill()\n",
    "\n",
    "        # Merge the dataframes on the timestamp columns\n",
    "        merged_df = pd.merge(df_labelled, df_recurrent_und[['Recurring Underperformance', 'is_low_cloudiness_day']], left_on='time', right_index=True, how='left')\n",
    "\n",
    "        # Rename the Recurring Underperformance column\n",
    "        merged_df.rename(columns={'Recurring Underperformance': 'fault_recurrent_underperformance'}, inplace=True)\n",
    "\n",
    "        # Now merged_df contains the df_genW data with the added Recurring Underperformance column\n",
    "        df_labelled = merged_df\n",
    "\n",
    "        # If needed, you can fill any NaN values in the new column with a default value (e.g., False)\n",
    "        df_labelled['fault_recurrent_underperformance'].fillna(False, inplace=True)\n",
    "\n",
    "        # =======================================#\n",
    "        # = Adding a column with array of faults #\n",
    "        # =======================================#\n",
    "        df_labelled['faults'] = df_labelled.apply(find_faults, axis=1)\n",
    "\n",
    "        # =============================#\n",
    "        # = Saving faults - every 5min #\n",
    "        # =============================#\n",
    "\n",
    "        # Saving the whole thing\n",
    "        df_labelled.to_csv(f'./2B_monitor_results/individual_monitors/{MID}.csv')\n",
    "\n",
    "\n",
    "        # Saving per day\n",
    "        df_per_day = df_labelled.copy()\n",
    "\n",
    "        df_per_day.index = df_per_day.index.date\n",
    "\n",
    "        def agg_faults_per_day(faults_list):\n",
    "            unique_faults_on_that_day = list(set([fault for sublist in faults_list for fault in sublist]))\n",
    "            return unique_faults_on_that_day\n",
    "\n",
    "        result_per_day = df_per_day.groupby(df_per_day.index).agg({'Gen.W': 'sum', 'faults': agg_faults_per_day})\n",
    "\n",
    "        result_per_day.to_csv(f'./2B_monitor_results/per_day/{MID}_per_day.csv')\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # =================#\n",
    "        # = Error Handling #\n",
    "        # =================#\n",
    "\n",
    "        # This is a generic error handling, as specific errors are found, they can be placed under their respective place\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
