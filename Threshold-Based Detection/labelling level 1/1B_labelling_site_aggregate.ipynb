{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Performance Analysis for ALL SITES\n",
    "\n",
    "This document runs a all sites through a performance ratio analsys.\n",
    "\n",
    "It uses this analyses to label the site (on that day) based on Level 1 faults."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Libraries import\n",
    "# ========================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import pytz\n",
    "# import math\n",
    "# from zoneinfo import ZoneInfo\n",
    "import datetime\n",
    "# import geopy.distance\n",
    "# from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import seaborn as sns\n",
    "from datetime import timedelta\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = AWS Credentials\n",
    "# ========================================================\n",
    "\n",
    "PROD_AWS_PROFILE = \"gsesami-prod\"\n",
    "AWS_REGION = \"ap-southeast-2\"\n",
    "\n",
    "prod_session = boto3.session.Session(profile_name=PROD_AWS_PROFILE)\n",
    "\n",
    "prod_client = prod_session.client(\n",
    "    \"timestream-query\", region_name=AWS_REGION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining the Site ID, and dates:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Reading site list and monitor list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all sites\n",
    "sites_list = pd.read_csv('./input_data/Site_List.csv')\n",
    "# Reading all monitors\n",
    "monitors_list = pd.read_csv('./input_data/Monitors_List.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Defining start and end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time period\n",
    "date_start = '2020-01-01'\n",
    "\n",
    "# Setting date_end to today\n",
    "today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "date_end = today"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Metrics to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining metrics to be read\n",
    "clear_sky = 'EnergyYield.kWh.Daily'\n",
    "expected = 'Irrad.kWh.m2.Daily'\n",
    "measured = 'Production.kWh.Daily'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_id(df, sequence):\n",
    "    site_id = df['source'].loc[sequence].removeprefix('SITE|')\n",
    "    return site_id\n",
    "\n",
    "def get_site_id_full(df, sequence):\n",
    "    site_id_full = df['source'].loc[sequence]\n",
    "    return site_id_full\n",
    "\n",
    "def get_site_name(df, sequence):\n",
    "    site_name = df['name'].loc[sequence]\n",
    "    return site_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_info(df, sequence):\n",
    "    site_id = get_site_id(df, sequence)\n",
    "    site_id_full = get_site_id_full(df, sequence)\n",
    "    # site_name = get_site_name(df, sequence)\n",
    "    return site_id, site_id_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_check(row):\n",
    "    if row['Performance.perc.Daily'] >= 80:\n",
    "        val = 'ok'\n",
    "    elif row['Performance.perc.Daily'] >=60:\n",
    "        val = 'medium'\n",
    "    else:\n",
    "        val = 'under'\n",
    "    return val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Defining thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Thresholds\n",
    "# ========================================================\n",
    "\n",
    "# Cloudiness\n",
    "# Define the threshold for low cloudiness days:\n",
    "threshold_low_cloudiness = 80\n",
    "\n",
    "# Parameters for checking major underperformance\n",
    "window_size_major_und = 3\n",
    "threshold_performance_major_und = 60\n",
    "\n",
    "# Parameters for checking minor underperformance\n",
    "window_size_minor_und = 7\n",
    "threshold_performance_minor_und = 80\n",
    "\n",
    "# Parameters for weekend and weekdays underperormance\n",
    "threshold_performance_weekends_weekdays = 20\n",
    "\n",
    "# Parameters for checking seasonal underperformance\n",
    "threshold_performance_seasonal = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Functions to fetch data from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metric_site(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(timeid, measure_name, data_values):\n",
    "    # ============== Check if there is data available for the pv system =============\n",
    "    if len(timeid)!=0:\n",
    "        timeid = pd.to_datetime(timeid)\n",
    "        if timeid.tzinfo is None:\n",
    "            print('this is not tz-aware')\n",
    "            if timezone_value is not None:\n",
    "                timeid = timeid.tz_localize('UTC').tz_convert(timezone_value)\n",
    "                # timeid = timeid.tz_localize(timezone_list[i])\n",
    "            else:\n",
    "                print('no timezone in the table')\n",
    "                timeid = timeid.tz_localize('UTC').tz_convert('Australia/Sydney')\n",
    "                # timeid = timeid.tz_localize('Australia/Sydney')\n",
    "        else:\n",
    "            print('this is tz-aware')\n",
    "        \n",
    "        timesort = timeid.sort_values()\n",
    "        data = pd.DataFrame(data={'time':timeid, measure_name: data_values})\n",
    "        data.sort_values('time', inplace=True)\n",
    "        data.set_index('time', inplace=True)\n",
    "        data[measure_name] = data[measure_name].astype(float)\n",
    "    else:\n",
    "        data = pd.DataFrame(data_values, index=timeid, columns=[measure_name])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Reading EnergyYield.kWh.Daily from AWS TimeStream\n",
    "# ========================================================\n",
    "\n",
    "def readClear(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Reading Irrad.kWh.m2.Daily from AWS TimeStream\n",
    "# ========================================================\n",
    "\n",
    "\n",
    "def readExpected(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Reading Production.kWh.Daily from AWS TimeStream\n",
    "# ========================================================\n",
    "\n",
    "def readMeasured(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"DiagnoProd\".\"DiagnoProd\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Functions to label level 1 faults based on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_major_underperformance(df, window_size, threshold):\n",
    "    # create a boolean mask to identify the rows where the 'Performance.perc.Daily' column is below 60 (or threshold)\n",
    "    # we'll use this to filter out the dataframe\n",
    "    mask = df['Performance.perc.Daily'] < threshold\n",
    "\n",
    "    # create a new column 'majpr_underperformance' with the default value 'FALSE'\n",
    "    df['major_underperformance'] = 'FALSE'\n",
    "\n",
    "    # label the rows where the 'Performance.perc.Daily' column has dropped below 60 for 3 consecutive days or more\n",
    "    # 1st, loop through the df:\n",
    "    for i in range((window_size-1), len(df)):\n",
    "        # check if the mask apply for N consecutive days (window_size):\n",
    "        if all(mask.iloc[i - j] for j in range(window_size)):\n",
    "            # Label it as such:\n",
    "            # Mark all days in the sequence as 'Minor underperformance'\n",
    "            ## Using window_size to do this retroactively:\n",
    "            for j in range(window_size):\n",
    "                df.loc[df.index[i-j], 'major_underperformance'] = 'Major underperformance'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_minor_underperformance(df, window_size, threshold):\n",
    "    # create a boolean mask to identify the rows where the 'Performance.perc.Daily' column is below the threshold\n",
    "    mask = df['Performance.perc.Daily'] < threshold\n",
    "\n",
    "    # create a new column 'minor_underperformance' with the default value 'FALSE'\n",
    "    df['minor_underperformance'] = 'FALSE'\n",
    "\n",
    "    # label the rows where the 'Performance.perc.Daily' column has dropped below the threshold for the specified number of consecutive days or more\n",
    "    # 1st, loop through the df:\n",
    "    for i in range((window_size-1), len(df)):\n",
    "        # check if the mask apply for N consecutive days (window_size):        \n",
    "        if mask.iloc[i] and all(mask.iloc[i-window_size+1:i+1]):\n",
    "            # Label it as such:\n",
    "            # Mark all days in the sequence as 'Minor underperformance'\n",
    "            ## Using window_size to do this retroactively:\n",
    "            for j in range(window_size):\n",
    "                df.loc[df.index[i-j], 'minor_underperformance'] = 'Minor underperformance'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if:\n",
    "# The performance of days in the current week (weekdays or weekends) \n",
    "# falls below the performance of the opposite type of days (weekends or weekdays) \n",
    "# from the previous week by a certain threshold. \n",
    "# If so, it labels them as underperforming. \n",
    "\n",
    "\n",
    "def check_week_performance(df, threshold, performance_col='Performance.perc.Daily'):\n",
    "    # create new columns for week and week performance labels\n",
    "    # using datetime to add week\n",
    "    df['Week'] = df.index.to_period('W').astype(str)\n",
    "    # initialising weekday average\n",
    "    df['Prev Weekday Avg'] = 0\n",
    "    # initialising weekend average\n",
    "    df['Prev Weekend Avg'] = 0\n",
    "    # initialising default week performance value (FALSE)\n",
    "    # this might be changed to either 'weekday underperformance' or 'weekend underperformance' (or remain unchanged)\n",
    "    df['week_underperformance'] = 'FALSE'\n",
    "\n",
    "    # calculating the weekday and weekend averages for each week\n",
    "    unique_weeks = df['Week'].unique()\n",
    "    for i, week in enumerate(unique_weeks):\n",
    "        if i == 0:  # skip the first week as there is no previous week to compare\n",
    "            continue\n",
    "\n",
    "        prev_week = unique_weeks[i - 1]\n",
    "        prev_week_df = df[df['Week'] == prev_week]\n",
    "\n",
    "        # for weekdays:\n",
    "        prev_weekday_avg = prev_week_df.loc[~prev_week_df['is_weekend'], performance_col].mean()\n",
    "        # for weekends:\n",
    "        prev_weekend_avg = prev_week_df.loc[prev_week_df['is_weekend'], performance_col].mean()\n",
    "\n",
    "        # label the rows where the performance is below the average for their respective day types against the previous week's daytypes\n",
    "\n",
    "        # For each day in the current week, the function checks:\n",
    "        ## If it's a weekday and its performance is below the previous week's weekend average minus the threshold, it labels it as 'Weekday underperformance'\n",
    "        ## If it's a weekend and its performance is below the previous week's weekday average minus the threshold, it labels it as 'Weekend underperformance'.\n",
    "        df.loc[(df['Week'] == week) & (~df['is_weekend']) & (df[performance_col] < prev_weekend_avg - threshold), 'week_underperformance'] = 'Weekday underperformance'\n",
    "        df.loc[(df['Week'] == week) & (df['is_weekend']) & (df[performance_col] < prev_weekday_avg - threshold), 'week_underperformance'] = 'Weekend underperformance'\n",
    "\n",
    "        # store the previous week's averages in the dataframe\n",
    "        df.loc[df['Week'] == week, 'Prev Weekday Avg'] = prev_weekday_avg\n",
    "        df.loc[df['Week'] == week, 'Prev Weekend Avg'] = prev_weekend_avg\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_seasonal_performance(df, threshold):\n",
    "    # Initialize the 'seasonal_fault' column with False\n",
    "    df['seasonal_underperformance'] = 'FALSE'\n",
    "    \n",
    "    # We want to check Seasonal underperfromance only after we have 1 year worth of data.\n",
    "    # We'll get check if the dataset has at leat 1 year worth of data:\n",
    "    ## Get the date of the oldest record in the dataset\n",
    "    oldest_date = df.index.min()\n",
    "    ## Get the current date and make it timezone-aware\n",
    "    current_date = df.index.max()\n",
    "    # Check if the oldest record is more than a year old\n",
    "    if (current_date - oldest_date) < timedelta(days=365):\n",
    "        print(\"Oldest data is less than a year old. The function needs data older than one year.\")\n",
    "        return df\n",
    "    \n",
    "\n",
    "    \n",
    "    # Get the average for each season\n",
    "    season_avg = df.groupby('season')['Performance.perc.Daily'].mean()\n",
    "    \n",
    "    # Create a new column 'season_avg_performance' with the average value for the corresponding season\n",
    "    df['season_avg_performance'] = df['season'].apply(lambda x: season_avg[x])\n",
    "    \n",
    "    # Calculate the sum of the averages for the other seasons\n",
    "    other_season_sum = {\n",
    "        'summer': (season_avg['autumn'] + season_avg['winter'] + season_avg['spring']) / 3,\n",
    "        'winter': (season_avg['autumn'] + season_avg['summer'] + season_avg['spring']) / 3\n",
    "    }\n",
    "    \n",
    "    # Check for underperformance and update the 'seasonal_fault' column accordingly\n",
    "    for index, row in df.iterrows():\n",
    "        season = row['season']\n",
    "        performance = row['Performance.perc.Daily']\n",
    "\n",
    "        # Only start labeling after a year has passed\n",
    "        if (index - oldest_date) < timedelta(days=365):\n",
    "            continue\n",
    "\n",
    "        if season == 'summer' and performance < other_season_sum['summer'] - threshold:\n",
    "            df.loc[index, 'seasonal_underperformance'] = 'summer underperformance'\n",
    "        elif season == 'winter' and performance < other_season_sum['winter'] - threshold:\n",
    "            df.loc[index, 'seasonal_underperformance'] = 'winter underperformance'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Labelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the array for the new column\n",
    "def create_array(row):\n",
    "    return [value for value in row[4:] if value != 'FALSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map the month to the season\n",
    "# Note that this has been done for Australia (Southern hemisphere)\n",
    "\n",
    "seasons = {1: 'summer', \n",
    "        2: 'summer', \n",
    "        3: 'autumn', \n",
    "        4: 'autumn', \n",
    "        5: 'autumn', \n",
    "        6: 'winter', \n",
    "        7: 'winter', \n",
    "        8: 'winter', \n",
    "        9: 'spring', \n",
    "        10: 'spring', \n",
    "        11: 'spring', \n",
    "        12: 'summer'\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Running the labelling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sites_list)):\n",
    "    try:\n",
    "        site_id, site_id_full = get_site_info(sites_list, i)\n",
    "        # Checking\n",
    "        print(\"Checking tz-aware for: \", site_id_full)\n",
    "\n",
    "        # Checking timezone\n",
    "        timezone_value = 'Australia/Sydney'\n",
    "        timezone_value = sites_list[sites_list['source'] == site_id_full].iloc[0]['timezone']\n",
    "\n",
    "        time_starttz = pytz.timezone('UTC').localize(datetime.datetime.strptime(date_start, '%Y-%m-%d'))\n",
    "        time_endtz = pytz.timezone('UTC').localize(datetime.datetime.strptime(date_end, '%Y-%m-%d'))\n",
    "\n",
    "        # ========================================================\n",
    "        # = Getting Clear sky \n",
    "        # ========================================================\n",
    "\n",
    "        measure_name = 'EnergyYield.kWh.Daily'\n",
    "        timeid, data_values = read_metric_site(date_start, date_end, measure_name, site_id)\n",
    "        df_clear = build_dataframe(timeid, measure_name, data_values)\n",
    "        df_clear['EnergyYield.kWh.Daily'] = df_clear['EnergyYield.kWh.Daily'].astype(float)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Getting expected generation\n",
    "        # ========================================================\n",
    "\n",
    "        measure_name = 'Irrad.kWh.m2.Daily'\n",
    "        timeid, data_values = read_metric_site(date_start, date_end, measure_name, site_id)\n",
    "        df_expected = build_dataframe(timeid, measure_name, data_values)\n",
    "        # Fixing it as a float:\n",
    "        df_expected['Irrad.kWh.m2.Daily'] = df_expected['Irrad.kWh.m2.Daily'].astype(float)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Merging clear skies and expected\n",
    "        # ========================================================\n",
    "\n",
    "        def merge_clear_expe(df1, df2):\n",
    "            df_merged = df1.join(df2)\n",
    "            df_merged['expected_over_clear'] =  (df_merged['Irrad.kWh.m2.Daily'] / df_merged['EnergyYield.kWh.Daily'] * 100).round(0)\n",
    "            df_merged['date'] =  df_merged.index\n",
    "            return df_merged\n",
    "\n",
    "        df_merged = merge_clear_expe(df_clear, df_expected)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Getting low cloudiness days\n",
    "        # ========================================================\n",
    "\n",
    "        df_merged.loc[df_merged['expected_over_clear'] >= threshold_low_cloudiness, 'is_low_clousdiness_day'] = True \n",
    "        df_merged.loc[df_merged['expected_over_clear'] < threshold_low_cloudiness, 'is_low_clousdiness_day'] = False\n",
    "\n",
    "        \n",
    "        # ==================================================\n",
    "        # = Reading Production.kWh.Daily from AWS TimeStream\n",
    "        # ==================================================\n",
    "\n",
    "        measure_name = 'Production.kWh.Daily'\n",
    "        timeid, data_values = read_metric_site(date_start, date_end, measure_name, site_id)\n",
    "        df_production = build_dataframe(timeid, measure_name, data_values)\n",
    "        # Fixing it as float\n",
    "        df_production['Production.kWh.Daily'] = df_production['Production.kWh.Daily'].astype(float)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Merging it and getting a % of performance daily\n",
    "        # ========================================================\n",
    "\n",
    "        df_performance = df_production.join(df_merged)\n",
    "        df_performance['Performance.perc.Daily'] = (df_performance['Production.kWh.Daily'] / df_performance['Irrad.kWh.m2.Daily'] * 100).round(0)\n",
    "        \n",
    "        df_performance['performancelabel'] = df_performance.apply(performance_check, axis=1)\n",
    "\n",
    "        # extract the day of the week using the weekday() method from dataframe\n",
    "        df_performance['day_of_week'] = df_performance['date'].apply(lambda x: x.weekday())\n",
    "\n",
    "        # create a binary indicator variable for weekends vs weekdays\n",
    "        df_performance['is_weekend'] = df_performance['day_of_week'].apply(lambda x: x in [5, 6])\n",
    "\n",
    "        # ========================================================\n",
    "        # = Adding weekend and seasonal info\n",
    "        # ========================================================\n",
    "\n",
    "        # extract the day of the week using the weekday() method from dataframe\n",
    "        df_performance['day_of_week'] = df_performance['date'].apply(lambda x: x.weekday())\n",
    "\n",
    "        # create a binary indicator variable for weekends vs weekdays\n",
    "        df_performance['is_weekend'] = df_performance['day_of_week'].apply(lambda x: x in [5, 6])\n",
    "\n",
    "        # Get the month using the datetime.month attribute\n",
    "        df_performance['month'] = df_performance['date'].dt.month\n",
    "\n",
    "        # Getting seasons using the dictionary created before\n",
    "        df_performance['season'] = df_performance['month'].apply(lambda x: seasons[x])\n",
    "\n",
    "        # ========================================================\n",
    "        # = Keeping only low cloudiness\n",
    "        # ========================================================\n",
    "\n",
    "        df_LC = df_performance[df_performance['is_low_clousdiness_day'] == True]\n",
    "\n",
    "        print(\"Checking level 1 performance-related faults for: \" + str(site_id_full))\n",
    "\n",
    "        # ========================================================\n",
    "        # = Labeling level 1 faults based on performance\n",
    "        # ========================================================\n",
    "                \n",
    "        check_major_underperformance(df_LC, window_size_major_und, threshold_performance_major_und)\n",
    "        check_minor_underperformance(df_LC, window_size_minor_und, threshold_performance_minor_und)\n",
    "        check_week_performance(df_LC, threshold_performance_weekends_weekdays)\n",
    "        check_seasonal_performance(df_LC, threshold_performance_seasonal)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Cleaning up the dataframe\n",
    "        # ========================================================\n",
    "\n",
    "        # Keep only the performance and label columns\n",
    "        level1_sites_labels_df = df_LC[[\n",
    "            'EnergyYield.kWh.Daily','Irrad.kWh.m2.Daily','Production.kWh.Daily','Performance.perc.Daily',\n",
    "            'major_underperformance', 'minor_underperformance', 'week_underperformance', 'seasonal_underperformance'\n",
    "            ]]\n",
    "        # Create the new 'level1-labels-site' column\n",
    "        level1_sites_labels_df['level1-labels-site'] = level1_sites_labels_df.apply(create_array, axis=1)\n",
    "\n",
    "        # Drop rows with empty arrays in 'level1-labels-site'\n",
    "        level1_sites_labels_df = level1_sites_labels_df.loc[level1_sites_labels_df['level1-labels-site'].apply(len) > 0]\n",
    "\n",
    "        level1_sites_labels_df.to_csv('./1B_site_results/individual_sites/' + str(site_id) + '.csv')\n",
    "\n",
    "        print(f'Analysis concluded for: {site_id_full}')\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Outputting a single CSV with all the faulty sites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining last rows:\n",
    "input_folder = './1B_site_results/individual_sites/'\n",
    "\n",
    "# List all files in the folder\n",
    "file_list = os.listdir(input_folder)\n",
    "csv_files = [file for file in file_list if file.endswith('.csv')]\n",
    "\n",
    "# Read the last row of each CSV file, add the 'site_id' column, and store it in a list\n",
    "last_rows = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # If the DataFrame is empty, skip this file\n",
    "    if df.empty:\n",
    "        continue\n",
    "    \n",
    "    # Extract site_id from the file name (assuming site_id is the entire file name without the .csv extension)\n",
    "    site_id = os.path.splitext(file)[0]\n",
    "\n",
    "    last_row = df.iloc[-1]\n",
    "    last_row['site_id'] = site_id\n",
    "    last_rows.append(last_row)\n",
    "\n",
    "# Create a DataFrame from the list of last rows\n",
    "combined_df = pd.DataFrame(last_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe just for TODAY:\n",
    "combined_df['time'] = pd.to_datetime(combined_df['time'], utc=True)\n",
    "combined_df['time'] = combined_df['time'].dt.tz_convert('Australia/Sydney')\n",
    "combined_df['time'] = combined_df['time'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_today_faults = combined_df[combined_df['time'] == today]\n",
    "df_today_faults.to_csv(f'./1B_site_results/aggregate/{today}_today_sites_level1faults.csv', index=False)\n",
    "\n",
    "df_today_faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe for a week ago:\n",
    "\n",
    "today_full = datetime.date.today()\n",
    "week_ago = today_full - datetime.timedelta(days=7)\n",
    "\n",
    "combined_df['time'] = pd.to_datetime(combined_df['time'], utc=True)\n",
    "combined_df['time'] = combined_df['time'].dt.tz_convert('Australia/Sydney')\n",
    "combined_df['time'] = combined_df['time'].dt.date\n",
    "\n",
    "# Make sure that 'today' is a date object, not a string.\n",
    "today = today_full  \n",
    "\n",
    "df_week_faults = combined_df[(combined_df['time'] >= week_ago) & (combined_df['time'] <= today)]\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = './1B_site_results/aggregate/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_week_faults.to_csv(f'{output_dir}{today}_lastWeekAggregate_sites_level1faults.csv', index=False)\n",
    "\n",
    "df_week_faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Filtering dataframe for a week ago:\n",
    "\n",
    "today_full = datetime.date.today()\n",
    "week_ago = today_full - datetime.timedelta(days=7)\n",
    "\n",
    "combined_df['time'] = pd.to_datetime(combined_df['time'], utc=True)\n",
    "combined_df['time'] = combined_df['time'].dt.tz_convert('Australia/Sydney')\n",
    "combined_df['time'] = combined_df['time'].dt.date\n",
    "\n",
    "# Make sure that 'today' is a date object, not a string.\n",
    "today = today_full  \n",
    "\n",
    "df_week_faults = combined_df[(combined_df['time'] >= week_ago) & (combined_df['time'] <= today)]\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = './1B_site_results/aggregate/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_week_faults.to_csv(f'{output_dir}{today}_lastWeekAggregate_sites_level1faults.csv', index=False)\n",
    "\n",
    "df_week_faults'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdcc8738350348f5da3cd958a6f31e3755dac4740b3163f526dc5114836e977f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
