{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Libraries import\n",
    "# ========================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import pytz\n",
    "import math\n",
    "from zoneinfo import ZoneInfo\n",
    "import datetime\n",
    "import geopy.distance\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = AWS Credentials\n",
    "# ========================================================\n",
    "\n",
    "PROD_AWS_PROFILE = \"gsesami-prod\"\n",
    "AWS_REGION = \"us-west-2\"\n",
    "\n",
    "prod_session = boto3.session.Session(profile_name=PROD_AWS_PROFILE)\n",
    "\n",
    "prod_client = prod_session.client(\n",
    "    \"timestream-query\", region_name=AWS_REGION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Getting SiteIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of sites:\n",
    "df_sites = pd.read_csv('./input_data/Site_List.csv')\n",
    "\n",
    "# To review:\n",
    "df_sites = df_sites[~df_sites['performanceSinceInceptionSortKey'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_sky = 'EnergyYield.kWh.Daily'\n",
    "expected = 'Irrad.kWh.m2.Daily'\n",
    "measured = 'Production.kWh.Daily'\n",
    "\n",
    "# ========================================================\n",
    "# = Define period\n",
    "# ========================================================\n",
    "\n",
    "date_start = '2022-01-01'\n",
    "date_end = '2023-02-01'\n",
    "\n",
    "# ========================================================\n",
    "# = Thresholds\n",
    "# ========================================================\n",
    "\n",
    "threshold_low_cloudiness = 80\n",
    "window_size = 7\n",
    "threshold_performance = -10\n",
    "threshold_underperformance_days = 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functions to build the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_id(df, sequence):\n",
    "    site_id = df['source'].loc[sequence].removeprefix('SITE|')\n",
    "    return site_id\n",
    "\n",
    "def get_site_id_full(df, sequence):\n",
    "    site_id_full = df['source'].loc[sequence]\n",
    "    return site_id_full\n",
    "\n",
    "def get_site_name(df, sequence):\n",
    "    site_name = df['name'].loc[sequence]\n",
    "    return site_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_info(df, sequence):\n",
    "    site_id = get_site_id(df, sequence)\n",
    "    site_id_full = get_site_id_full(df, sequence)\n",
    "    # site_name = get_site_name(df, sequence)\n",
    "    return site_id, site_id_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_check(row):\n",
    "    if row['Performance.perc.Daily'] >= 80:\n",
    "        val = 'ok'\n",
    "    elif row['Performance.perc.Daily'] >=60:\n",
    "        val = 'medium'\n",
    "    else:\n",
    "        val = 'under'\n",
    "    return val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Functions to fetch data from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Reading EnergyYield.kWh.Daily from AWS TimeStream\n",
    "# ========================================================\n",
    "\n",
    "def readClear(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"GSESTimeseries\".\"GSESTimeseriesTable\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Reading Irrad.kWh.m2.Daily from AWS TimeStream\n",
    "# ========================================================\n",
    "\n",
    "\n",
    "def readExpected(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"GSESTimeseries\".\"GSESTimeseriesTable\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# = Reading Production.kWh.Daily from AWS TimeStream\n",
    "# ========================================================\n",
    "\n",
    "def readMeasured(date_start, date_end, measure_name, site_id):\n",
    "    timeid = []\n",
    "    data_values = []\n",
    "    ##----------------- read the Performance  --------------##\n",
    "    query = \"\"\"SELECT date, max_by(measure_value::double, time) as prod_val\n",
    "                FROM \"GSESTimeseries\".\"GSESTimeseriesTable\"\n",
    "                WHERE measure_name = '\"\"\" + measure_name + \"\"\"'\n",
    "                AND siteId = '\"\"\" + site_id + \"\"\"'\n",
    "                AND date BETWEEN '\"\"\" + date_start + \"\"\"'\n",
    "                AND '\"\"\" + date_end + \"\"\"'\n",
    "                GROUP BY date\n",
    "                ORDER BY date \"\"\"\n",
    "    \n",
    "    client = prod_client\n",
    "    paginator = client.get_paginator(\"query\")\n",
    "    page_iterator = paginator.paginate(QueryString=query,)\n",
    "    i = 1\n",
    "    for page in page_iterator:\n",
    "        # print(page)\n",
    "        try:\n",
    "            timeid_page = [f[0]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            data_values_page = [f[1]['ScalarValue'] for f in pd.DataFrame(page[\"Rows\"])['Data']]\n",
    "            timeid = timeid + timeid_page\n",
    "            data_values = data_values + data_values_page\n",
    "        except KeyError:\n",
    "            print('Page {%d} has no data available:'%i)\n",
    "        i = i+1\n",
    "    return timeid, data_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Functions to check faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_average(df, window_size):\n",
    "    df['SMA'] = df['Performance.perc.Daily'].rolling(window_size).mean()\n",
    "    return df\n",
    "\n",
    "def add_comparative (df):\n",
    "    df['comparative'] = np.nan\n",
    "    for i in range(len(df)):\n",
    "        df['comparative'].iloc[i] = (df['Performance.perc.Daily'].iloc[i] - df['SMA'].iloc[i])\n",
    "    return df\n",
    "\n",
    "def underperformance_check(df, threshold):\n",
    "    df['underperforming'] = np.nan\n",
    "    for i in range(len(df)):\n",
    "        df[\"underperforming\"].iloc[i] = df['comparative'].iloc[i] < threshold\n",
    "    return df\n",
    "\n",
    "def rolling_underperformance(df, days):\n",
    "    df['countUnder'] = np.nan\n",
    "    # Rolling count of underperforming days:\n",
    "    ix = pd.Series(range(df.shape[0])).where((~df['underperforming']).values, np.nan).ffill().values\n",
    "    notna = pd.notna(ix)\n",
    "    df[\"countUnder\"] = df[notna].groupby(ix[notna]).cumcount()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating SMA based on TRUE values of unpderforming\n",
    "def compare_underperfDay_with_SMA_of_under(df):\n",
    "    df['comparative_of_under'] = np.nan\n",
    "    steps_to_shift = 0\n",
    "    for i in range(len(df)):\n",
    "        if df['performancelabel'][i] == 'under':\n",
    "            steps_to_shift = steps_to_shift + 1\n",
    "            df['comparative_of_under'].iloc[i] = (df['Performance.perc.Daily'].iloc[i] - df['SMA'].iloc[i-steps_to_shift])\n",
    "            df['SMA'].iloc[i] = df['SMA'].iloc[i-steps_to_shift]\n",
    "        else:\n",
    "            steps_to_shift = 0\n",
    "            df['comparative_of_under'].iloc[i] = (df['Performance.perc.Daily'].iloc[i] - df['SMA'].iloc[i])\n",
    "    return df\n",
    "\n",
    "def underperformance_check_of_under(df, threshold):\n",
    "    df['underperforming_of_under'] = np.nan\n",
    "    for i in range(len(df)):\n",
    "        df[\"underperforming_of_under\"].iloc[i] = df['comparative_of_under'].iloc[i] < threshold\n",
    "    return df\n",
    "\n",
    "def rolling_underperformance_of_under(df, days):\n",
    "    df['countTrue_of_under'] = np.nan\n",
    "    # Rolling count of underperforming days:\n",
    "    ix = pd.Series(range(df.shape[0])).where((~df['underperforming_of_under']).values, np.nan).ffill().values\n",
    "    notna = pd.notna(ix)\n",
    "    df[\"countTrue_of_under\"] = df[notna].groupby(ix[notna]).cumcount()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retro_persistent_fault_check(df, days):\n",
    "    fault_sites = []\n",
    "    dates_fault_started = []\n",
    "    fault_details = np.empty\n",
    "    faulty_df = df[df['countTrue_of_under'] >= days]\n",
    "    sudden_unresolved = False\n",
    "    if not faulty_df.empty:\n",
    "        dates_fault_started = faulty_df[faulty_df['countTrue_of_under'] == days]['date']\n",
    "        count_fault = faulty_df.count()[0]\n",
    "        sudden_unresolved = True\n",
    "        print(\n",
    "            'Sudden and unresolved fault detected at '\n",
    "            + site_id_full\n",
    "            + '\\nDates in which fault started are:' \n",
    "            + dates_fault_started\n",
    "            + '\\n' \n",
    "            + '\\nTotal days of fault = '\n",
    "            +  str(count_fault)\n",
    "            )\n",
    "        # fault_details.append(site_id_full, site_name, sudden_unresolved, count_fault, faulty_df, dates_fault_started)\n",
    "        fault_sites.append(site_id_full)\n",
    "        return site_id_full, sudden_unresolved, count_fault, faulty_df, dates_fault_started\n",
    "    else:\n",
    "        sudden_unresolved = False\n",
    "        print(\"No long persistant faults detected at \" + site_id_full)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Checking long and persistent faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_sites = []\n",
    "fault_dict = {}\n",
    "\n",
    "for i in range(len(df_sites)):\n",
    "    try:\n",
    "        site_id, site_id_full = get_site_info(df_sites, i)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Getting Clear sky and expected generation values\n",
    "        # ========================================================\n",
    "        timeid, data_values = readClear(date_start, date_end, clear_sky, site_id)\n",
    "\n",
    "        df_clear = pd.DataFrame(data_values, index=timeid, columns=[clear_sky])\n",
    "        df_clear['EnergyYield.kWh.Daily'] = df_clear['EnergyYield.kWh.Daily'].astype(float)\n",
    "\n",
    "        timeid, data_values = readExpected(date_start, date_end, expected, site_id)\n",
    "\n",
    "        df_expected = pd.DataFrame(data_values, index=timeid, columns=[expected])\n",
    "        df_expected['Irrad.kWh.m2.Daily'] = df_expected['Irrad.kWh.m2.Daily'].astype(float)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Merging clear skies and expected\n",
    "        # ========================================================\n",
    "\n",
    "        def merge_clear_expe(df1, df2):\n",
    "            df_merged = df1.join(df2)\n",
    "            df_merged['expected_over_clear'] =  (df_merged['Irrad.kWh.m2.Daily'] / df_merged['EnergyYield.kWh.Daily'] * 100).round(0)\n",
    "            df_merged['date'] =  df_merged.index\n",
    "            return df_merged\n",
    "\n",
    "        df_merged = merge_clear_expe(df_clear, df_expected)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Getting low cloudiness days\n",
    "        # ========================================================\n",
    "\n",
    "        df_merged.loc[df_merged['expected_over_clear'] >= threshold_low_cloudiness, 'is_low_clousdiness_day'] = True \n",
    "        df_merged.loc[df_merged['expected_over_clear'] < threshold_low_cloudiness, 'is_low_clousdiness_day'] = False\n",
    "\n",
    "        # ========================================================\n",
    "        # = Reading Production.kWh.Daily from AWS TimeStream\n",
    "        # ========================================================\n",
    "\n",
    "        timeid, data_values = readMeasured(date_start, date_end, measured, site_id)\n",
    "\n",
    "        df_production = pd.DataFrame(data_values, index=timeid, columns=[measured])\n",
    "        df_production['Production.kWh.Daily'] = df_production['Production.kWh.Daily'].astype(float)\n",
    "\n",
    "        # ========================================================\n",
    "        # = Merging it and getting a % of performance daily\n",
    "        # ========================================================\n",
    "\n",
    "        df_performance = df_production.join(df_merged)\n",
    "        df_performance['Performance.perc.Daily'] = (df_performance['Production.kWh.Daily'] / df_performance['Irrad.kWh.m2.Daily'] * 100).round(0)\n",
    "        \n",
    "        df_performance['performancelabel'] = df_performance.apply(performance_check, axis=1)\n",
    "\n",
    "        df_LC = df_performance[df_performance['is_low_clousdiness_day'] == True]\n",
    "\n",
    "        # df_LC.to_csv('./input_data/sites_stored_locally/' + str(site_id) + '_' + str(site_id_full) + '.csv')\n",
    "\n",
    "        print(\"Checking persistant faults for: \" + str(site_id_full))\n",
    "\n",
    "        # Absolute analysis:\n",
    "        get_rolling_average(df_LC, window_size)\n",
    "        add_comparative(df_LC)\n",
    "        underperformance_check(df_LC, threshold_performance)\n",
    "        rolling_underperformance(df_LC, threshold_underperformance_days)\n",
    "\n",
    "        # Analysis on days that underperformed, excluding such days from the rolling average:\n",
    "        compare_underperfDay_with_SMA_of_under(df_LC)\n",
    "        underperformance_check_of_under(df_LC, threshold_performance)\n",
    "        rolling_underperformance_of_under(df_LC, threshold_underperformance_days)\n",
    "\n",
    "        fault_sites.append(retro_persistent_fault_check(df_LC, threshold_underperformance_days))\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(e)\n",
    "\n",
    "print(fault_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of faulty days\n",
    "df_faulty = pd.DataFrame(columns=['site_id','site_name','sudden_unresolved','faulty_days', 'faulty_df', 'dates_fault_started'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe with not-none faulty sites\n",
    "fault_res = [i for i in fault_sites if i is not None]\n",
    "\n",
    "for i in range(len(fault_res)):\n",
    "    df_faulty.loc[i] = [fault_res[i][0],fault_res[i][1],fault_res[i][2],fault_res[i][3],fault_res[i][4],fault_res[i][5]]\n",
    "df_faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving it to a CSV\n",
    "df_faulty.to_csv('SiteIDs_with_sudden_and_unresolved.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdcc8738350348f5da3cd958a6f31e3755dac4740b3163f526dc5114836e977f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
